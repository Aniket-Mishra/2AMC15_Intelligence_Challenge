{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37cd82fe",
   "metadata": {},
   "source": [
    "# Setup Experimental Table from experiment_values.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf9ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef3eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"experiment_values.json\"\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "defaults = config[\"defaults\"]\n",
    "grids = config[\"grids\"]\n",
    "experiments = config[\"experiments\"]\n",
    "\n",
    "all_param_names = set()\n",
    "for agent_params in defaults.values():\n",
    "    all_param_names.update(agent_params.keys())\n",
    "\n",
    "rows = []\n",
    "\n",
    "for agent, sweep in experiments.items():\n",
    "    default_params = defaults[agent]\n",
    "\n",
    "    for grid in grids:\n",
    "        for param_name, values in sweep.items():\n",
    "            for val in values:\n",
    "                params = deepcopy(default_params)\n",
    "                params[param_name] = val\n",
    "\n",
    "                row = {\n",
    "                    \"agent\": agent,\n",
    "                    \"grid\": grid,\n",
    "                    \"param_changed\": param_name,\n",
    "                    \"param_value\": val\n",
    "                }\n",
    "\n",
    "                for pname in sorted(all_param_names):\n",
    "                    row[pname] = params.get(pname, float('nan'))\n",
    "\n",
    "                rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df = df.loc[df[\"agent\"] != \"RandomAgent\"]\n",
    "df.to_csv(\"experiment_results/experiment_table.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26988342",
   "metadata": {},
   "source": [
    "# Run Experiment\n",
    "\n",
    "## This is a copy of train.py with minor updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaaab05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import json, io, sys, re, importlib, inspect\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "from world.reward_functions import custom_reward_function\n",
    "from world import Environment\n",
    "from agents import BaseAgent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3203b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agent(agent_name: str, env: Environment, config: dict):\n",
    "    info = config[agent_name]\n",
    "    mod  = importlib.import_module(info[\"module\"])\n",
    "    cls  = getattr(mod, info[\"class\"])\n",
    "    init_args = info.get(\"init_args\", {})\n",
    "    sig = inspect.signature(cls.__init__)\n",
    "    if 'env' in sig.parameters:\n",
    "        return cls(env=env, **init_args), info[\"train_mode\"]\n",
    "    else:\n",
    "        return cls(**init_args), info[\"train_mode\"]\n",
    "\n",
    "def update_agent(agent: BaseAgent, args: Namespace, state, next_state, reward, action):\n",
    "    params = inspect.signature(agent.update).parameters\n",
    "    names  = set(params)\n",
    "    if {\"state\",\"next_state\"}.issubset(names):\n",
    "        agent.update(state=state, next_state=next_state, reward=reward, action=action)\n",
    "    elif {\"next_state\",\"reward\",\"action\"}.issubset(names):\n",
    "        agent.update(next_state=next_state, reward=reward, action=action)\n",
    "    elif {\"state\",\"reward\",\"action\"}.issubset(names):\n",
    "        agent.update(state=state, reward=reward, action=action)\n",
    "    else:\n",
    "        agent.update()\n",
    "\n",
    "def train_and_eval(args: Namespace, config: dict):\n",
    "    start = tuple(args.agent_start_pos)\n",
    "    for grid_fp in args.GRID:\n",
    "        env = Environment(\n",
    "            Path(grid_fp),\n",
    "            args.no_gui,\n",
    "            sigma=args.sigma,\n",
    "            agent_start_pos=start,\n",
    "            reward_fn=custom_reward_function,\n",
    "            target_fps=args.fps,\n",
    "            random_seed=args.random_seed\n",
    "        )\n",
    "        env.reset()\n",
    "        agent, mode = load_agent(args.agent, env, config)\n",
    "\n",
    "        if mode == \"q_learning\":\n",
    "            #Max difference for convergence check\n",
    "            metrics = {\"iterations\": 0, \"deltas\": [], \"rewards\": []}\n",
    "            delta = 1e-6\n",
    "            \n",
    "            for ep in trange(args.episodes, desc=f\"Training {args.agent}\"):\n",
    "                # Save a copy of the current Q-table for convergence check\n",
    "                prev_q_table = {\n",
    "                    s: np.copy(q_values) for s, q_values in agent.q_table.items()\n",
    "                }\n",
    "                state = env.reset()\n",
    "                ep_reward = 0.0\n",
    "                for _ in range(args.iter):\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, terminated, info = env.step(action)\n",
    "                    ep_reward += reward\n",
    "                    if terminated:\n",
    "                        break\n",
    "                    agent.update(state, next_state, reward, info[\"actual_action\"])\n",
    "                    state = next_state\n",
    "                \n",
    "                if ep >= args.episodes/4:\n",
    "                    agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "                    agent.alpha = max(agent.alpha_min, agent.alpha * agent.alpha_decay)\n",
    "\n",
    "                common_states = set(agent.q_table.keys()) & set(prev_q_table.keys())\n",
    "                if not common_states:\n",
    "                    max_diff = 1\n",
    "                else:\n",
    "                    max_diff = max(\n",
    "                        np.max(np.abs(agent.q_table[s] - prev_q_table[s]))\n",
    "                        for s in common_states\n",
    "                    )\n",
    "                metrics[\"deltas\"].append(max_diff)\n",
    "                metrics[\"rewards\"].append(ep_reward)\n",
    "                if max_diff < delta:\n",
    "                    metrics[\"iterations\"] = ep\n",
    "                    break\n",
    "\n",
    "            if metrics[\"iterations\"] == 0:\n",
    "                metrics[\"iterations\"] = args.episodes\n",
    "\n",
    "            agent.metrics = metrics\n",
    "    \n",
    "            agent.eval_mode()\n",
    "\n",
    "        elif mode == \"value_iteration\":\n",
    "            state = env.reset()\n",
    "            for _ in trange(args.iter, desc=f\"[Train] {args.agent}\"):\n",
    "                a  = agent.take_action(state)\n",
    "                ns, r, done, info = env.step(a)\n",
    "                update_agent(agent, args, state, ns, r, info[\"actual_action\"])\n",
    "                state = ns\n",
    "                if done: break\n",
    "\n",
    "        elif mode == \"monte_carlo\":\n",
    "            delta = 1e-6\n",
    "\n",
    "            metrics = {\"iterations\": 0, \"deltas\": [], \"rewards\": []}\n",
    "\n",
    "            for episode in trange(args.episodes, desc=f\"Training {args.agent}\"):\n",
    "                prev_q = {s: np.copy(agent.q_table[s]) for s in agent.q_table}\n",
    "\n",
    "                state = env.reset()\n",
    "                terminated = False\n",
    "                ep_reward = 0.0\n",
    "                for _ in range(args.iter):\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, terminated, info = env.step(action)\n",
    "                    ep_reward += reward\n",
    "                    if terminated:\n",
    "                        break\n",
    "                    agent.update(state, action, reward, next_state, False)\n",
    "                    state = next_state\n",
    "\n",
    "                agent.update(state, action, reward, next_state, True)\n",
    "\n",
    "                if episode >= args.episodes/4:\n",
    "                    agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "                    agent.alpha = max(agent.alpha_min, agent.alpha * agent.alpha_decay)\n",
    "\n",
    "                # Convergence check\n",
    "                common_states = set(agent.q_table.keys()) & set(prev_q.keys())\n",
    "                if not common_states:\n",
    "                    max_diff = 1\n",
    "                else:\n",
    "                    max_diff = max(\n",
    "                        np.max(np.abs(agent.q_table[s] - prev_q[s]))\n",
    "                        for s in common_states\n",
    "                    )\n",
    "\n",
    "                metrics[\"deltas\"].append(max_diff)\n",
    "                metrics[\"rewards\"].append(ep_reward)\n",
    "\n",
    "                if max_diff < delta:\n",
    "                    metrics[\"iterations\"] = episode\n",
    "                    break\n",
    "\n",
    "            if metrics[\"iterations\"] == 0:\n",
    "                metrics[\"iterations\"] = args.episodes\n",
    "\n",
    "            agent.metrics = metrics\n",
    "            agent.epsilon = 0.0  # Switch to greedy\n",
    "\n",
    "        else:  # iterative / random\n",
    "            state = env.reset()\n",
    "            for _ in trange(args.iter, desc=f\"[Train] {args.agent}\"):\n",
    "                a = agent.take_action(state)\n",
    "                ns, r, done, info = env.step(a)\n",
    "                update_agent(agent, args, state, ns, r, info[\"actual_action\"])\n",
    "                state = ns\n",
    "                if done: break\n",
    "\n",
    "    # capture evaluation output\n",
    "    buf = io.StringIO()\n",
    "    old_out, old_err = sys.stdout, sys.stderr\n",
    "    sys.stdout, sys.stderr = buf, buf\n",
    "    try:\n",
    "        Environment.evaluate_agent(\n",
    "            Path(args.GRID[0]),\n",
    "            agent,\n",
    "            args.iter,\n",
    "            args.sigma,\n",
    "            agent_start_pos=start,\n",
    "            reward_fn=custom_reward_function,\n",
    "            random_seed=args.random_seed,\n",
    "            show_images=False\n",
    "        )\n",
    "    finally:\n",
    "        sys.stdout, sys.stderr = old_out, old_err\n",
    "\n",
    "    text = buf.getvalue()\n",
    "    metrics = {}\n",
    "    for line in text.splitlines():\n",
    "        m = re.match(r\"\\s*([a-z_]+)\\s*:\\s*([-+]?[0-9]*\\.?[0-9]+)\", line)\n",
    "        if m:\n",
    "            k, v = m.group(1), m.group(2)\n",
    "            metrics[k] = int(v) if v.isdigit() else float(v)\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e64b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: QLearningAgent on A1_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  19%|█▉        | 387/2000 [00:00<00:00, 1898.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: QLearningAgent on A1_grid | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  24%|██▍       | 481/2000 [00:00<00:00, 2052.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: QLearningAgent on A1_grid | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  17%|█▋        | 335/2000 [00:00<00:00, 2395.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: QLearningAgent on A1_grid | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   9%|▉         | 186/2000 [00:00<00:00, 2260.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5: QLearningAgent on A1_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  20%|██        | 401/2000 [00:00<00:00, 1697.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6: QLearningAgent on A1_grid | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  24%|██▍       | 489/2000 [00:00<00:00, 2203.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: QLearningAgent on A1_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  26%|██▌       | 514/2000 [00:00<00:00, 2278.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8: QLearningAgent on A1_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  17%|█▋        | 341/2000 [00:00<00:01, 1533.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9: QLearningAgent on A1_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  10%|▉         | 479/5000 [00:00<00:02, 2187.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: QLearningAgent on A1_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▍       | 497/2000 [00:00<00:00, 2320.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11: QLearningAgent on Maze | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  24%|██▍       | 482/2000 [00:00<00:01, 1315.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12: QLearningAgent on Maze | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  26%|██▋       | 525/2000 [00:00<00:01, 1208.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13: QLearningAgent on Maze | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▍       | 491/2000 [00:00<00:01, 1392.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14: QLearningAgent on Maze | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  13%|█▎        | 257/2000 [00:00<00:01, 1385.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15: QLearningAgent on Maze | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  20%|██        | 407/2000 [00:00<00:01, 863.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: QLearningAgent on Maze | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  22%|██▏       | 447/2000 [00:00<00:01, 1317.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17: QLearningAgent on Maze | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▌       | 509/2000 [00:00<00:01, 1431.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18: QLearningAgent on Maze | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  20%|█▉        | 390/2000 [00:00<00:01, 834.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19: QLearningAgent on Maze | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  15%|█▍        | 746/5000 [00:00<00:02, 1599.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: QLearningAgent on Maze | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  26%|██▌       | 521/2000 [00:00<00:00, 1495.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21: QLearningAgent on test_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 73/2000 [00:00<00:00, 4471.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22: QLearningAgent on test_grid | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 73/2000 [00:00<00:00, 4563.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23: QLearningAgent on test_grid | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   2%|▏         | 47/2000 [00:00<00:00, 4217.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24: QLearningAgent on test_grid | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   1%|▏         | 26/2000 [00:00<00:02, 920.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25: QLearningAgent on test_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   3%|▎         | 66/2000 [00:00<00:00, 3004.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26: QLearningAgent on test_grid | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 73/2000 [00:00<00:00, 4264.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27: QLearningAgent on test_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▍         | 75/2000 [00:00<00:00, 4958.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28: QLearningAgent on test_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▍         | 78/2000 [00:00<00:00, 3710.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29: QLearningAgent on test_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   2%|▏         | 76/5000 [00:00<00:01, 4143.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30: QLearningAgent on test_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 72/2000 [00:00<00:00, 4407.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31: QLearningAgent on large_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  11%|█         | 222/2000 [00:00<00:01, 1533.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32: QLearningAgent on large_grid | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  10%|█         | 206/2000 [00:00<00:01, 1564.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33: QLearningAgent on large_grid | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   9%|▉         | 181/2000 [00:00<00:01, 1670.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34: QLearningAgent on large_grid | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▍         | 86/2000 [00:00<00:01, 1487.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35: QLearningAgent on large_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  16%|█▌        | 316/2000 [00:00<00:01, 1353.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36: QLearningAgent on large_grid | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  11%|█         | 220/2000 [00:00<00:01, 1602.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37: QLearningAgent on large_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  11%|█         | 222/2000 [00:00<00:01, 1655.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38: QLearningAgent on large_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  13%|█▎        | 262/2000 [00:00<00:01, 1377.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39: QLearningAgent on large_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▍         | 195/5000 [00:00<00:03, 1553.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40: QLearningAgent on large_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  11%|█▏        | 229/2000 [00:00<00:01, 1592.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41: ValueIterationAgent on A1_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 16/2000 [00:00<00:00, 74071.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42: ValueIterationAgent on A1_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 15/2000 [00:00<00:00, 72817.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43: ValueIterationAgent on A1_grid | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 15/2000 [00:00<00:00, 68759.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44: ValueIterationAgent on A1_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 11/2000 [00:00<00:00, 58180.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45: ValueIterationAgent on A1_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 23/2000 [00:00<00:00, 91962.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46: ValueIterationAgent on A1_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 15/5000 [00:00<00:00, 64067.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47: ValueIterationAgent on Maze | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent: 100%|██████████| 2000/2000 [00:00<00:00, 138590.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48: ValueIterationAgent on Maze | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   2%|▏         | 36/2000 [00:00<00:00, 83932.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49: ValueIterationAgent on Maze | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   2%|▏         | 36/2000 [00:00<00:00, 101135.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50: ValueIterationAgent on Maze | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   2%|▏         | 33/2000 [00:00<00:00, 104461.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51: ValueIterationAgent on Maze | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:  52%|█████▏    | 1047/2000 [00:00<00:00, 138086.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52: ValueIterationAgent on Maze | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 36/5000 [00:00<00:00, 112935.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53: ValueIterationAgent on test_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 23258.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54: ValueIterationAgent on test_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 25627.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55: ValueIterationAgent on test_grid | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 19722.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56: ValueIterationAgent on test_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 2/2000 [00:00<00:00, 16131.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57: ValueIterationAgent on test_grid | sigma=0.5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 5/2000 [00:00<00:00, 33825.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58: ValueIterationAgent on test_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/5000 [00:00<00:00, 12495.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59: ValueIterationAgent on large_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 80659.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60: ValueIterationAgent on large_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 77172.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61: ValueIterationAgent on large_grid | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 59833.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62: ValueIterationAgent on large_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 16/2000 [00:00<00:00, 78766.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63: ValueIterationAgent on large_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|▏         | 29/2000 [00:00<00:00, 104407.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64: ValueIterationAgent on large_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 20/5000 [00:00<00:00, 87381.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65: MCAgentOn on A1_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  94%|█████████▍| 1890/2000 [00:06<00:00, 273.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66: MCAgentOn on A1_grid | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  95%|█████████▍| 1899/2000 [00:00<00:00, 2097.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67: MCAgentOn on A1_grid | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  91%|█████████ | 1811/2000 [00:00<00:00, 2748.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68: MCAgentOn on A1_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  86%|████████▋ | 1730/2000 [00:00<00:00, 2934.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69: MCAgentOn on A1_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  78%|███████▊  | 1560/2000 [00:00<00:00, 2703.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70: MCAgentOn on A1_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:01<00:00, 1588.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71: MCAgentOn on A1_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  52%|█████▏    | 2610/5000 [00:01<00:00, 2554.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72: MCAgentOn on A1_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:01<00:00, 1742.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73: MCAgentOn on Maze | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  95%|█████████▍| 1897/2000 [00:06<00:00, 282.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74: MCAgentOn on Maze | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:07<00:00, 269.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75: MCAgentOn on Maze | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:07<00:00, 267.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76: MCAgentOn on Maze | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:07<00:00, 283.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77: MCAgentOn on Maze | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:07<00:00, 278.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78: MCAgentOn on Maze | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:07<00:00, 250.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79: MCAgentOn on Maze | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 5000/5000 [00:18<00:00, 266.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80: MCAgentOn on Maze | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:18<00:00, 105.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81: MCAgentOn on test_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  85%|████████▍ | 1693/2000 [00:00<00:00, 8355.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82: MCAgentOn on test_grid | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  80%|████████  | 1607/2000 [00:00<00:00, 8754.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83: MCAgentOn on test_grid | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  81%|████████  | 1624/2000 [00:00<00:00, 10153.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84: MCAgentOn on test_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  84%|████████▍ | 1676/2000 [00:00<00:00, 7824.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85: MCAgentOn on test_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  83%|████████▎ | 1666/2000 [00:00<00:00, 13461.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86: MCAgentOn on test_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  89%|████████▊ | 1772/2000 [00:00<00:00, 5258.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87: MCAgentOn on test_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  48%|████▊     | 2380/5000 [00:00<00:00, 8246.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88: MCAgentOn on test_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  81%|████████▏ | 1629/2000 [00:00<00:00, 8669.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89: MCAgentOn on large_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:07<00:00, 273.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90: MCAgentOn on large_grid | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:01<00:00, 1116.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91: MCAgentOn on large_grid | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:04<00:00, 410.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92: MCAgentOn on large_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:01<00:00, 1415.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93: MCAgentOn on large_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:03<00:00, 583.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94: MCAgentOn on large_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:04<00:00, 496.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95: MCAgentOn on large_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  55%|█████▌    | 2752/5000 [00:03<00:02, 860.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96: MCAgentOn on large_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:11<00:00, 168.30it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"experiment_results/experimental_table.csv\")\n",
    "base_cfg = json.load(open(\"agent_config.json\"))\n",
    "exp_defs = json.load(open(\"experiment_values.json\"))\n",
    "\n",
    "rows = []\n",
    "for idx, row in df.iterrows():\n",
    "    agent = row[\"agent\"]\n",
    "    grid  = row[\"grid\"]\n",
    "    print(f\"{idx+1}: {agent} on {grid} | {row['param_changed']}={row['param_value']}\")\n",
    "\n",
    "    init_args, cli_args = {}, {}\n",
    "    for c,v in row.items():\n",
    "        if pd.isna(v) or c in {\"agent\",\"grid\",\"param_changed\",\"param_value\"}:\n",
    "            continue\n",
    "        if c in {\"episodes\",\"iter\"}:\n",
    "            cli_args[c] = int(v)\n",
    "        elif c == \"sigma\":\n",
    "            cli_args[c] = float(v)\n",
    "        else:\n",
    "            init_args[c] = float(v)\n",
    "\n",
    "    cfg = deepcopy(base_cfg)\n",
    "    defaults_init = cfg[agent].get(\"init_args\", {})\n",
    "    cfg[agent][\"init_args\"] = {**defaults_init, **init_args}\n",
    "\n",
    "    default_sigma = exp_defs[\"defaults\"][agent].get(\"sigma\", 0.0)\n",
    "    sigma = cli_args.get(\"sigma\", default_sigma)\n",
    "\n",
    "    ns = Namespace(\n",
    "        GRID=[f\"grid_configs/{grid}.npy\"],\n",
    "        agent=agent,\n",
    "        no_gui=True,\n",
    "        sigma=sigma,\n",
    "        fps=5,\n",
    "        episodes=cli_args.get(\"episodes\",\n",
    "                              exp_defs[\"defaults\"][agent].get(\"episodes\",2000)),\n",
    "        iter=cli_args.get(\"iter\",\n",
    "                          exp_defs[\"defaults\"][agent].get(\"iter\",2000)),\n",
    "        random_seed=42,\n",
    "        agent_start_pos=[1,1]\n",
    "    )\n",
    "\n",
    "    metrics = train_and_eval(ns, cfg)\n",
    "\n",
    "    result = row.to_dict()\n",
    "    result.update(metrics)\n",
    "    result[\"sigma\"] = sigma\n",
    "    rows.append(result)\n",
    "\n",
    "out_df = pd.DataFrame(rows)\n",
    "out_df.to_csv(\"result_multi_experiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ead34b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (285669689.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mSTOP CODE\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "STOP CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb90598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent</th>\n",
       "      <th>grid</th>\n",
       "      <th>param_changed</th>\n",
       "      <th>param_value</th>\n",
       "      <th>alpha</th>\n",
       "      <th>episodes</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>epsilon_decay</th>\n",
       "      <th>gamma</th>\n",
       "      <th>iter</th>\n",
       "      <th>sigma</th>\n",
       "      <th>theta</th>\n",
       "      <th>cumulative_reward</th>\n",
       "      <th>total_steps</th>\n",
       "      <th>total_agent_moves</th>\n",
       "      <th>total_failed_moves</th>\n",
       "      <th>total_targets_reached</th>\n",
       "      <th>targets_remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>gamma</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>gamma</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>epsilon</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>epsilon</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-528.0</td>\n",
       "      <td>99</td>\n",
       "      <td>19</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>sigma</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-14000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>sigma</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9578.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>737</td>\n",
       "      <td>1263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>episodes</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-122.0</td>\n",
       "      <td>41</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>iter</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22822.0</td>\n",
       "      <td>3493</td>\n",
       "      <td>263</td>\n",
       "      <td>3230</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             agent        grid param_changed  param_value  alpha  episodes  \\\n",
       "0   QLearningAgent     A1_grid         gamma         0.60    0.2    2000.0   \n",
       "1   QLearningAgent     A1_grid         gamma         0.99    0.2    2000.0   \n",
       "2   QLearningAgent     A1_grid         alpha         0.30    0.3    2000.0   \n",
       "3   QLearningAgent     A1_grid         alpha         0.50    0.5    2000.0   \n",
       "4   QLearningAgent     A1_grid       epsilon         0.50    0.2    2000.0   \n",
       "..             ...         ...           ...          ...    ...       ...   \n",
       "91       MCAgentOn  large_grid       epsilon         0.50    NaN    2000.0   \n",
       "92       MCAgentOn  large_grid         sigma         0.00    NaN    2000.0   \n",
       "93       MCAgentOn  large_grid         sigma         0.50    NaN    2000.0   \n",
       "94       MCAgentOn  large_grid      episodes      5000.00    NaN    5000.0   \n",
       "95       MCAgentOn  large_grid          iter      5000.00    NaN    2000.0   \n",
       "\n",
       "    epsilon  epsilon_decay  gamma  iter  sigma  theta  cumulative_reward  \\\n",
       "0       0.2            0.9   0.60  2000    0.1    NaN               28.0   \n",
       "1       0.2            0.9   0.99  2000    0.1    NaN               28.0   \n",
       "2       0.2            0.9   0.90  2000    0.1    NaN               28.0   \n",
       "3       0.2            0.9   0.90  2000    0.1    NaN               35.0   \n",
       "4       0.5            0.9   0.90  2000    0.1    NaN               28.0   \n",
       "..      ...            ...    ...   ...    ...    ...                ...   \n",
       "91      0.5            NaN   0.99  2000    0.1    NaN             -528.0   \n",
       "92      0.1            NaN   0.99  2000    0.0    NaN           -14000.0   \n",
       "93      0.1            NaN   0.99  2000    0.5    NaN            -9578.0   \n",
       "94      0.1            NaN   0.99  2000    0.1    NaN             -122.0   \n",
       "95      0.1            NaN   0.99  5000    0.1    NaN           -22822.0   \n",
       "\n",
       "    total_steps  total_agent_moves  total_failed_moves  total_targets_reached  \\\n",
       "0            17                 16                   1                      1   \n",
       "1            17                 16                   1                      1   \n",
       "2            17                 16                   1                      1   \n",
       "3            16                 16                   0                      1   \n",
       "4            17                 16                   1                      1   \n",
       "..          ...                ...                 ...                    ...   \n",
       "91           99                 19                  80                      1   \n",
       "92         2000                  0                2000                      0   \n",
       "93         2000                737                1263                      0   \n",
       "94           41                 19                  22                      1   \n",
       "95         3493                263                3230                      1   \n",
       "\n",
       "    targets_remaining  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "..                ...  \n",
       "91                  0  \n",
       "92                  1  \n",
       "93                  1  \n",
       "94                  0  \n",
       "95                  0  \n",
       "\n",
       "[96 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
