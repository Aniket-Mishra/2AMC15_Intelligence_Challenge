{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37cd82fe",
   "metadata": {},
   "source": [
    "# Setup Experimental Table from experiment_values.json\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "edf9ae46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:51:39.277843Z",
     "start_time": "2025-05-16T10:51:39.275569Z"
    }
   },
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "aef3eca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:51:39.324192Z",
     "start_time": "2025-05-16T10:51:39.315855Z"
    }
   },
   "source": [
    "CONFIG_PATH = \"experiment_values.json\"\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "defaults = config[\"defaults\"]\n",
    "grids = config[\"grids\"]\n",
    "experiments = config[\"experiments\"]\n",
    "\n",
    "all_param_names = set()\n",
    "for agent_params in defaults.values():\n",
    "    all_param_names.update(agent_params.keys())\n",
    "\n",
    "rows = []\n",
    "\n",
    "for agent, sweep in experiments.items():\n",
    "    default_params = defaults[agent]\n",
    "\n",
    "    for grid in grids:\n",
    "        for param_name, values in sweep.items():\n",
    "            for val in values:\n",
    "                params = deepcopy(default_params)\n",
    "                params[param_name] = val\n",
    "\n",
    "                row = {\n",
    "                    \"agent\": agent,\n",
    "                    \"grid\": grid,\n",
    "                    \"param_changed\": param_name,\n",
    "                    \"param_value\": val\n",
    "                }\n",
    "\n",
    "                for pname in sorted(all_param_names):\n",
    "                    row[pname] = params.get(pname, float('nan'))\n",
    "\n",
    "                rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df = df.loc[df[\"agent\"] != \"RandomAgent\"]\n",
    "df.to_csv(\"experiment_results/experiment_table.csv\", index = False)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "26988342",
   "metadata": {},
   "source": [
    "# Run Experiment\n",
    "\n",
    "## This is a copy of train.py with minor updates\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aaaab05e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:51:39.357610Z",
     "start_time": "2025-05-16T10:51:39.354838Z"
    }
   },
   "source": [
    "import json, io, sys, re, importlib, inspect\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "from world.reward_functions import custom_reward_function\n",
    "from world import Environment\n",
    "from agents import BaseAgent\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "3203b3ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:51:39.385516Z",
     "start_time": "2025-05-16T10:51:39.371298Z"
    }
   },
   "source": [
    "import os, json, datetime, random\n",
    "\n",
    "def load_agent(agent_name: str, env: Environment, config: dict):\n",
    "    info = config[agent_name]\n",
    "    mod  = importlib.import_module(info[\"module\"])\n",
    "    cls  = getattr(mod, info[\"class\"])\n",
    "    init_args = info.get(\"init_args\", {})\n",
    "    sig = inspect.signature(cls.__init__)\n",
    "    if 'env' in sig.parameters:\n",
    "        return cls(env=env, **init_args), info[\"train_mode\"], info[\"init_args\"]\n",
    "    else:\n",
    "        return cls(**init_args), info[\"train_mode\"], info[\"init_args\"]\n",
    "\n",
    "def update_agent(agent: BaseAgent, args: Namespace, state, next_state, reward, action):\n",
    "    params = inspect.signature(agent.update).parameters\n",
    "    names  = set(params)\n",
    "    if {\"state\",\"next_state\"}.issubset(names):\n",
    "        agent.update(state=state, next_state=next_state, reward=reward, action=action)\n",
    "    elif {\"next_state\",\"reward\",\"action\"}.issubset(names):\n",
    "        agent.update(next_state=next_state, reward=reward, action=action)\n",
    "    elif {\"state\",\"reward\",\"action\"}.issubset(names):\n",
    "        agent.update(state=state, reward=reward, action=action)\n",
    "    else:\n",
    "        agent.update()\n",
    "\n",
    "def train_and_eval(args: Namespace, config: dict):\n",
    "    start = tuple(args.agent_start_pos)\n",
    "\n",
    "    for grid_fp in args.GRID:\n",
    "        env = Environment(\n",
    "            Path(grid_fp),\n",
    "            args.no_gui,\n",
    "            sigma=args.sigma,\n",
    "            agent_start_pos=start,\n",
    "            reward_fn=custom_reward_function,\n",
    "            target_fps=args.fps,\n",
    "            random_seed=args.random_seed\n",
    "        )\n",
    "        env.reset()\n",
    "        agent, mode, init_args = load_agent(args.agent, env, config)\n",
    "\n",
    "        if mode == \"q_learning\":\n",
    "            #Max difference for convergence check\n",
    "            metrics = {\"iterations\": 0, \"steps_taken\": 0, \"deltas\": [], \"rewards\": []}\n",
    "            delta = 1e-6\n",
    "\n",
    "            for ep in trange(args.episodes, desc=f\"Training {args.agent}\"):\n",
    "                # Save a copy of the current Q-table for convergence check\n",
    "                prev_q_table = {\n",
    "                    s: np.copy(q_values) for s, q_values in agent.q_table.items()\n",
    "                }\n",
    "                state = env.reset()\n",
    "                ep_reward = 0.0\n",
    "                for _ in range(args.iter):\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, terminated, info = env.step(action)\n",
    "                    ep_reward += reward\n",
    "                    if terminated:\n",
    "                        break\n",
    "                    agent.update(state, next_state, reward, info[\"actual_action\"])\n",
    "                    state = next_state\n",
    "\n",
    "                if ep >= args.episodes/4:\n",
    "                    agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "                    agent.alpha = max(agent.alpha_min, agent.alpha * agent.alpha_decay)\n",
    "\n",
    "                common_states = set(agent.q_table.keys()) & set(prev_q_table.keys())\n",
    "                if not common_states:\n",
    "                    max_diff = 1\n",
    "                else:\n",
    "                    max_diff = max(\n",
    "                        np.max(np.abs(agent.q_table[s] - prev_q_table[s]))\n",
    "                        for s in common_states\n",
    "                    )\n",
    "                metrics[\"deltas\"].append(max_diff)\n",
    "                metrics[\"rewards\"].append(ep_reward)\n",
    "                metrics[\"steps_taken\"] = env.world_stats[\"total_steps\"]\n",
    "\n",
    "                if max_diff < delta:\n",
    "                    metrics[\"iterations\"] = ep\n",
    "                    break\n",
    "\n",
    "            if metrics[\"iterations\"] == 0:\n",
    "                metrics[\"iterations\"] = args.episodes\n",
    "\n",
    "            agent.metrics = metrics\n",
    "\n",
    "            agent.eval_mode()\n",
    "\n",
    "        elif mode == \"value_iteration\":\n",
    "            state = env.reset()\n",
    "            for _ in trange(args.iter, desc=f\"[Train] {args.agent}\"):\n",
    "                a  = agent.take_action(state)\n",
    "                ns, r, done, info = env.step(a)\n",
    "                update_agent(agent, args, state, ns, r, info[\"actual_action\"])\n",
    "                state = ns\n",
    "                if done: break\n",
    "            agent.metrics[\"steps_taken\"] = env.world_stats[\"total_steps\"]\n",
    "\n",
    "        elif mode == \"monte_carlo\":\n",
    "            delta = 1e-6\n",
    "\n",
    "            metrics = {\"iterations\": 0, \"steps_taken\": 0, \"deltas\": [], \"rewards\": []}\n",
    "\n",
    "            for episode in trange(args.episodes, desc=f\"Training {args.agent}\"):\n",
    "                prev_q = {s: np.copy(agent.q_table[s]) for s in agent.q_table}\n",
    "\n",
    "                state = env.reset()\n",
    "                terminated = False\n",
    "                ep_reward = 0.0\n",
    "                for _ in range(args.iter):\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, terminated, info = env.step(action)\n",
    "                    ep_reward += reward\n",
    "                    if terminated:\n",
    "                        break\n",
    "                    agent.update(state, action, reward, next_state, False)\n",
    "                    state = next_state\n",
    "\n",
    "                agent.update(state, action, reward, next_state, True)\n",
    "\n",
    "                if episode >= args.episodes/4:\n",
    "                    agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "                    agent.alpha = max(agent.alpha_min, agent.alpha * agent.alpha_decay)\n",
    "\n",
    "                # Convergence check\n",
    "                common_states = set(agent.q_table.keys()) & set(prev_q.keys())\n",
    "                if not common_states:\n",
    "                    max_diff = 1\n",
    "                else:\n",
    "                    max_diff = max(\n",
    "                        np.max(np.abs(agent.q_table[s] - prev_q[s]))\n",
    "                        for s in common_states\n",
    "                    )\n",
    "\n",
    "                metrics[\"deltas\"].append(max_diff)\n",
    "                metrics[\"rewards\"].append(ep_reward)\n",
    "                metrics[\"steps_taken\"] = env.world_stats[\"total_steps\"]\n",
    "\n",
    "                if max_diff < delta:\n",
    "                    metrics[\"iterations\"] = episode\n",
    "                    break\n",
    "\n",
    "            if metrics[\"iterations\"] == 0:\n",
    "                metrics[\"iterations\"] = args.episodes\n",
    "\n",
    "            agent.metrics = metrics\n",
    "            agent.epsilon = 0.0  # Switch to greedy\n",
    "\n",
    "        else:  # iterative / random\n",
    "            state = env.reset()\n",
    "            for _ in trange(args.iter, desc=f\"[Train] {args.agent}\"):\n",
    "                a = agent.take_action(state)\n",
    "                ns, r, done, info = env.step(a)\n",
    "                update_agent(agent, args, state, ns, r, info[\"actual_action\"])\n",
    "                state = ns\n",
    "                if done: break\n",
    "\n",
    "        if hasattr(agent, \"metrics\"):\n",
    "            its = agent.metrics.get(\"iterations\", None)\n",
    "            print(f\"[Metrics] {args.agent} converged in {its} iterations\")\n",
    "            metrics_dir = \"metrics\"\n",
    "            os.makedirs(metrics_dir, exist_ok=True)\n",
    "            grid_name = Path(grid).stem  # Extract just the filename without extension\n",
    "            param_str = \"_\".join(f\"{k}-{v}\" for k, v in init_args.items())\n",
    "            fname = f\"{args.agent}_grid-{grid_name}_{param_str}_{random.randrange(1,9999)}.json\"\n",
    "\n",
    "            path = os.path.join(metrics_dir, fname)\n",
    "        try:\n",
    "            with open(path, \"w\") as mf:\n",
    "                json.dump(agent.metrics, mf, indent=2)\n",
    "                print(f\"[Metrics] Saved convergence data to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Metrics] ERROR saving metrics: {e}\")\n",
    "\n",
    "\n",
    "    # capture evaluation output\n",
    "    buf = io.StringIO()\n",
    "    old_out, old_err = sys.stdout, sys.stderr\n",
    "    sys.stdout, sys.stderr = buf, buf\n",
    "    try:\n",
    "        Environment.evaluate_agent(\n",
    "            Path(args.GRID[0]),\n",
    "            agent,\n",
    "            args.iter,\n",
    "            args.sigma,\n",
    "            agent_start_pos=start,\n",
    "            reward_fn=custom_reward_function,\n",
    "            random_seed=args.random_seed,\n",
    "            show_images=False\n",
    "        )\n",
    "    finally:\n",
    "        sys.stdout, sys.stderr = old_out, old_err\n",
    "\n",
    "    text = buf.getvalue()\n",
    "    metrics = {}\n",
    "    for line in text.splitlines():\n",
    "        m = re.match(r\"\\s*([a-z_]+)\\s*:\\s*([-+]?[0-9]*\\.?[0-9]+)\", line)\n",
    "        if m:\n",
    "            k, v = m.group(1), m.group(2)\n",
    "            metrics[k] = int(v) if v.isdigit() else float(v)\n",
    "    return metrics\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "48e64b4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:56:27.574924Z",
     "start_time": "2025-05-16T10:51:39.394727Z"
    }
   },
   "source": [
    "\n",
    "df = pd.read_csv(\"experiment_results/experimental_table.csv\")\n",
    "base_cfg = json.load(open(\"agent_config.json\"))\n",
    "exp_defs = json.load(open(\"experiment_values.json\"))\n",
    "\n",
    "rows = []\n",
    "for idx, row in df.iterrows():\n",
    "    agent = row[\"agent\"]\n",
    "    grid  = row[\"grid\"]\n",
    "    print(f\"{idx+1}: {agent} on {grid} | {row['param_changed']}={row['param_value']}\")\n",
    "\n",
    "    init_args, cli_args = {}, {}\n",
    "    for c,v in row.items():\n",
    "        if pd.isna(v) or c in {\"agent\",\"grid\",\"param_changed\",\"param_value\"}:\n",
    "            continue\n",
    "        if c in {\"episodes\",\"iter\"}:\n",
    "            cli_args[c] = int(v)\n",
    "        elif c == \"sigma\":\n",
    "            cli_args[c] = float(v)\n",
    "        else:\n",
    "            init_args[c] = float(v)\n",
    "\n",
    "    cfg = deepcopy(base_cfg)\n",
    "    defaults_init = cfg[agent].get(\"init_args\", {})\n",
    "    cfg[agent][\"init_args\"] = {**defaults_init, **init_args}\n",
    "\n",
    "    default_sigma = exp_defs[\"defaults\"][agent].get(\"sigma\", 0.0)\n",
    "    sigma = cli_args.get(\"sigma\", default_sigma)\n",
    "\n",
    "    ns = Namespace(\n",
    "        GRID=[f\"grid_configs/{grid}.npy\"],\n",
    "        agent=agent,\n",
    "        no_gui=True,\n",
    "        sigma=sigma,\n",
    "        fps=5,\n",
    "        episodes=cli_args.get(\"episodes\",\n",
    "                              exp_defs[\"defaults\"][agent].get(\"episodes\",2000)),\n",
    "        iter=cli_args.get(\"iter\",\n",
    "                          exp_defs[\"defaults\"][agent].get(\"iter\",2000)),\n",
    "        random_seed=42,\n",
    "        agent_start_pos=[1,1]\n",
    "    )\n",
    "\n",
    "    metrics = train_and_eval(ns, cfg)\n",
    "\n",
    "    result = row.to_dict()\n",
    "    result.update(metrics)\n",
    "    result[\"sigma\"] = sigma\n",
    "    rows.append(result)\n",
    "\n",
    "out_df = pd.DataFrame(rows)\n",
    "out_df.to_csv(\"result_multi_experiment.csv\", index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: QLearningAgent on A1_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  20%|█▉        | 393/2000 [00:00<00:02, 695.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 393 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.6_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_3215.json\n",
      "2: QLearningAgent on A1_grid | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▍       | 498/2000 [00:00<00:01, 807.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 498 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.99_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_1951.json\n",
      "3: QLearningAgent on A1_grid | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  17%|█▋        | 335/2000 [00:00<00:02, 784.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 335 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.3_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_2403.json\n",
      "4: QLearningAgent on A1_grid | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  10%|▉         | 190/2000 [00:00<00:02, 782.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 190 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.5_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6892.json\n",
      "5: QLearningAgent on A1_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  19%|█▉        | 386/2000 [00:00<00:02, 590.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 386 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.5_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7740.json\n",
      "6: QLearningAgent on A1_grid | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  24%|██▍       | 484/2000 [00:00<00:01, 794.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 484 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.8_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6496.json\n",
      "7: QLearningAgent on A1_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▌       | 505/2000 [00:00<00:01, 838.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 505 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7501.json\n",
      "8: QLearningAgent on A1_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  20%|█▉        | 396/2000 [00:00<00:02, 647.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 396 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_8672.json\n",
      "9: QLearningAgent on A1_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   9%|▉         | 460/5000 [00:00<00:05, 819.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 460 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6488.json\n",
      "10: QLearningAgent on A1_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  24%|██▍       | 480/2000 [00:00<00:01, 804.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 480 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_3423.json\n",
      "11: QLearningAgent on Maze | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▌       | 508/2000 [00:00<00:02, 523.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 508 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.6_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6113.json\n",
      "12: QLearningAgent on Maze | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  28%|██▊       | 570/2000 [00:01<00:02, 562.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 570 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.99_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_1392.json\n",
      "13: QLearningAgent on Maze | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  23%|██▎       | 463/2000 [00:00<00:02, 587.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 463 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.3_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7424.json\n",
      "14: QLearningAgent on Maze | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  13%|█▎        | 255/2000 [00:00<00:03, 522.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 255 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.5_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6358.json\n",
      "15: QLearningAgent on Maze | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  22%|██▏       | 448/2000 [00:01<00:04, 369.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 448 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.5_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_4614.json\n",
      "16: QLearningAgent on Maze | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  26%|██▌       | 512/2000 [00:00<00:02, 559.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 512 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.8_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_1284.json\n",
      "17: QLearningAgent on Maze | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▌       | 509/2000 [00:00<00:02, 566.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 509 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_9691.json\n",
      "18: QLearningAgent on Maze | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  19%|█▉        | 387/2000 [00:01<00:04, 379.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 387 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_8798.json\n",
      "19: QLearningAgent on Maze | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  16%|█▌        | 794/5000 [00:01<00:06, 616.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 794 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_4291.json\n",
      "20: QLearningAgent on Maze | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  26%|██▌       | 523/2000 [00:00<00:02, 542.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 523 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_4762.json\n",
      "21: QLearningAgent on test_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   3%|▎         | 62/2000 [00:00<00:01, 1480.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 62 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.6_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_9450.json\n",
      "22: QLearningAgent on test_grid | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 74/2000 [00:00<00:01, 1590.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 74 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.99_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7658.json\n",
      "23: QLearningAgent on test_grid | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   2%|▏         | 47/2000 [00:00<00:01, 1448.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 47 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.3_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_2590.json\n",
      "24: QLearningAgent on test_grid | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   1%|▏         | 25/2000 [00:00<00:01, 1560.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 25 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.5_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_1495.json\n",
      "25: QLearningAgent on test_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 70/2000 [00:00<00:01, 1236.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 70 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.5_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7735.json\n",
      "26: QLearningAgent on test_grid | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   3%|▎         | 69/2000 [00:00<00:01, 1438.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 69 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.8_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_4337.json\n",
      "27: QLearningAgent on test_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 74/2000 [00:00<00:01, 1632.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 74 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_8585.json\n",
      "28: QLearningAgent on test_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   3%|▎         | 69/2000 [00:00<00:01, 1275.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 69 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_324.json\n",
      "29: QLearningAgent on test_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   2%|▏         | 75/5000 [00:00<00:03, 1431.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 75 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_9631.json\n",
      "30: QLearningAgent on test_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 71/2000 [00:00<00:01, 1576.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 71 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6522.json\n",
      "31: QLearningAgent on large_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   8%|▊         | 150/2000 [00:00<00:03, 497.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 150 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.6_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_8311.json\n",
      "32: QLearningAgent on large_grid | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  11%|█         | 222/2000 [00:00<00:03, 554.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 222 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.99_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_1732.json\n",
      "33: QLearningAgent on large_grid | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   9%|▉         | 176/2000 [00:00<00:02, 617.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 176 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.3_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_2187.json\n",
      "34: QLearningAgent on large_grid | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   6%|▋         | 128/2000 [00:00<00:02, 654.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 128 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.5_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_9515.json\n",
      "35: QLearningAgent on large_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  14%|█▍        | 289/2000 [00:00<00:03, 500.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 289 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.5_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6796.json\n",
      "36: QLearningAgent on large_grid | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  12%|█▏        | 239/2000 [00:00<00:02, 603.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 239 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.8_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_2638.json\n",
      "37: QLearningAgent on large_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  13%|█▎        | 260/2000 [00:00<00:02, 644.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 260 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_2737.json\n",
      "38: QLearningAgent on large_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  13%|█▎        | 266/2000 [00:00<00:03, 516.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 266 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_1742.json\n",
      "39: QLearningAgent on large_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   5%|▍         | 239/5000 [00:00<00:07, 618.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 239 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_8973.json\n",
      "40: QLearningAgent on large_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  15%|█▌        | 300/2000 [00:00<00:02, 646.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 300 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_216.json\n",
      "41: ValueIterationAgent on A1_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 16/2000 [00:00<00:00, 30504.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 25 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-06_gamma-0.6_107.json\n",
      "42: ValueIterationAgent on A1_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 15/2000 [00:00<00:00, 27618.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 185 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-06_gamma-0.95_4558.json\n",
      "43: ValueIterationAgent on A1_grid | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 15/2000 [00:00<00:00, 25795.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 82 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-05_gamma-0.9_4558.json\n",
      "44: ValueIterationAgent on A1_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 11/2000 [00:00<00:00, 27962.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 91 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-06_gamma-0.9_435.json\n",
      "45: ValueIterationAgent on A1_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 23/2000 [00:00<00:00, 30269.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 99 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-06_gamma-0.9_8786.json\n",
      "46: ValueIterationAgent on A1_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 15/5000 [00:00<00:00, 25742.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 93 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-06_gamma-0.9_4558.json\n",
      "47: ValueIterationAgent on Maze | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent: 100%|██████████| 2000/2000 [00:00<00:00, 66531.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 24 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-06_gamma-0.6_6228.json\n",
      "48: ValueIterationAgent on Maze | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   2%|▏         | 36/2000 [00:00<00:00, 45248.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 192 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-06_gamma-0.95_9460.json\n",
      "49: ValueIterationAgent on Maze | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   2%|▏         | 36/2000 [00:00<00:00, 44189.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 89 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-05_gamma-0.9_9460.json\n",
      "50: ValueIterationAgent on Maze | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   2%|▏         | 33/2000 [00:00<00:00, 35049.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 97 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-06_gamma-0.9_6202.json\n",
      "51: ValueIterationAgent on Maze | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:  52%|█████▏    | 1047/2000 [00:00<00:00, 63440.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 105 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-06_gamma-0.9_1989.json\n",
      "52: ValueIterationAgent on Maze | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 36/5000 [00:00<00:00, 38053.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 100 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-06_gamma-0.9_9460.json\n",
      "53: ValueIterationAgent on test_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 12336.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 21 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-06_gamma-0.6_1680.json\n",
      "54: ValueIterationAgent on test_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 14513.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 182 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-06_gamma-0.95_1680.json\n",
      "55: ValueIterationAgent on test_grid | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 11155.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 78 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-05_gamma-0.9_1680.json\n",
      "56: ValueIterationAgent on test_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 2/2000 [00:00<00:00, 6477.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 87 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-06_gamma-0.9_3658.json\n",
      "57: ValueIterationAgent on test_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 5/2000 [00:00<00:00, 16617.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 99 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-06_gamma-0.9_1425.json\n",
      "58: ValueIterationAgent on test_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/5000 [00:00<00:00, 9946.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 90 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-06_gamma-0.9_1680.json\n",
      "59: ValueIterationAgent on large_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 37718.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 28 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-06_gamma-0.6_4553.json\n",
      "60: ValueIterationAgent on large_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 37068.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 187 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-06_gamma-0.95_4553.json\n",
      "61: ValueIterationAgent on large_grid | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 42560.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 85 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-05_gamma-0.9_4553.json\n",
      "62: ValueIterationAgent on large_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 16/2000 [00:00<00:00, 30685.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 94 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-06_gamma-0.9_9655.json\n",
      "63: ValueIterationAgent on large_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|▏         | 29/2000 [00:00<00:00, 43224.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 105 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-06_gamma-0.9_9460.json\n",
      "64: ValueIterationAgent on large_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 20/5000 [00:00<00:00, 40079.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 96 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-06_gamma-0.9_4553.json\n",
      "65: MCAgentOn on A1_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  94%|█████████▍| 1890/2000 [00:13<00:00, 143.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1890 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.95_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_2381.json\n",
      "66: MCAgentOn on A1_grid | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  95%|█████████▍| 1899/2000 [00:02<00:00, 912.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1899 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.999_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_9148.json\n",
      "67: MCAgentOn on A1_grid | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  91%|█████████ | 1811/2000 [00:01<00:00, 1074.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1811 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.2_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_1758.json\n",
      "68: MCAgentOn on A1_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  86%|████████▋ | 1730/2000 [00:01<00:00, 1115.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1730 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.5_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_41.json\n",
      "69: MCAgentOn on A1_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  78%|███████▊  | 1560/2000 [00:01<00:00, 1091.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1560 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_6953.json\n",
      "70: MCAgentOn on A1_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:02<00:00, 725.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_3890.json\n",
      "71: MCAgentOn on A1_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  52%|█████▏    | 2610/5000 [00:02<00:02, 1054.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2610 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_2310.json\n",
      "72: MCAgentOn on A1_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:02<00:00, 809.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_5466.json\n",
      "73: MCAgentOn on Maze | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  95%|█████████▍| 1897/2000 [00:13<00:00, 142.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1897 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.95_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_5349.json\n",
      "74: MCAgentOn on Maze | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:14<00:00, 142.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.999_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_680.json\n",
      "75: MCAgentOn on Maze | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:14<00:00, 142.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.2_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_508.json\n",
      "76: MCAgentOn on Maze | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:14<00:00, 138.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.5_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_1616.json\n",
      "77: MCAgentOn on Maze | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:14<00:00, 142.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_5024.json\n",
      "78: MCAgentOn on Maze | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:15<00:00, 129.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_6356.json\n",
      "79: MCAgentOn on Maze | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 5000/5000 [00:37<00:00, 133.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 5000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_8705.json\n",
      "80: MCAgentOn on Maze | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:34<00:00, 58.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_4846.json\n",
      "81: MCAgentOn on test_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  85%|████████▍ | 1693/2000 [00:00<00:00, 2602.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1693 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.95_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_860.json\n",
      "82: MCAgentOn on test_grid | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  80%|████████  | 1607/2000 [00:00<00:00, 2677.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1607 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.999_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_130.json\n",
      "83: MCAgentOn on test_grid | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  81%|████████  | 1624/2000 [00:00<00:00, 2925.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1624 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.2_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_2986.json\n",
      "84: MCAgentOn on test_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  84%|████████▍ | 1676/2000 [00:00<00:00, 2504.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1676 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.5_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_8548.json\n",
      "85: MCAgentOn on test_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  83%|████████▎ | 1666/2000 [00:00<00:00, 3582.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1666 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_5531.json\n",
      "86: MCAgentOn on test_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  89%|████████▊ | 1772/2000 [00:00<00:00, 1895.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1772 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_6772.json\n",
      "87: MCAgentOn on test_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  48%|████▊     | 2380/5000 [00:00<00:01, 2557.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2380 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_451.json\n",
      "88: MCAgentOn on test_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  81%|████████▏ | 1629/2000 [00:00<00:00, 2660.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1629 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_8797.json\n",
      "89: MCAgentOn on large_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:13<00:00, 143.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.95_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_8682.json\n",
      "90: MCAgentOn on large_grid | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:03<00:00, 534.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.999_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_9512.json\n",
      "91: MCAgentOn on large_grid | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:09<00:00, 211.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.2_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_9148.json\n",
      "92: MCAgentOn on large_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:03<00:00, 623.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.5_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_4548.json\n",
      "93: MCAgentOn on large_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:06<00:00, 297.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_4298.json\n",
      "94: MCAgentOn on large_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:07<00:00, 260.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_5027.json\n",
      "95: MCAgentOn on large_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  55%|█████▌    | 2752/5000 [00:06<00:05, 442.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2752 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_1681.json\n",
      "96: MCAgentOn on large_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:21<00:00, 91.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_4343.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "bbb90598",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:56:43.917337Z",
     "start_time": "2025-05-16T10:56:43.900811Z"
    }
   },
   "source": [
    "out_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             agent        grid param_changed  param_value  alpha  episodes  \\\n",
       "0   QLearningAgent     A1_grid         gamma         0.60    0.2    2000.0   \n",
       "1   QLearningAgent     A1_grid         gamma         0.99    0.2    2000.0   \n",
       "2   QLearningAgent     A1_grid         alpha         0.30    0.3    2000.0   \n",
       "3   QLearningAgent     A1_grid         alpha         0.50    0.5    2000.0   \n",
       "4   QLearningAgent     A1_grid       epsilon         0.50    0.2    2000.0   \n",
       "..             ...         ...           ...          ...    ...       ...   \n",
       "91       MCAgentOn  large_grid       epsilon         0.50    NaN    2000.0   \n",
       "92       MCAgentOn  large_grid         sigma         0.00    NaN    2000.0   \n",
       "93       MCAgentOn  large_grid         sigma         0.50    NaN    2000.0   \n",
       "94       MCAgentOn  large_grid      episodes      5000.00    NaN    5000.0   \n",
       "95       MCAgentOn  large_grid          iter      5000.00    NaN    2000.0   \n",
       "\n",
       "    epsilon  epsilon_decay  gamma  iter  sigma  theta  cumulative_reward  \\\n",
       "0       0.2            0.9   0.60  2000    0.1    NaN               35.0   \n",
       "1       0.2            0.9   0.99  2000    0.1    NaN               28.0   \n",
       "2       0.2            0.9   0.90  2000    0.1    NaN               35.0   \n",
       "3       0.2            0.9   0.90  2000    0.1    NaN               28.0   \n",
       "4       0.5            0.9   0.90  2000    0.1    NaN               35.0   \n",
       "..      ...            ...    ...   ...    ...    ...                ...   \n",
       "91      0.5            NaN   0.99  2000    0.1    NaN             -528.0   \n",
       "92      0.1            NaN   0.99  2000    0.0    NaN           -14000.0   \n",
       "93      0.1            NaN   0.99  2000    0.5    NaN            -9578.0   \n",
       "94      0.1            NaN   0.99  2000    0.1    NaN             -122.0   \n",
       "95      0.1            NaN   0.99  5000    0.1    NaN           -22822.0   \n",
       "\n",
       "    total_steps  total_agent_moves  total_failed_moves  total_targets_reached  \\\n",
       "0            16                 16                   0                      1   \n",
       "1            17                 16                   1                      1   \n",
       "2            16                 16                   0                      1   \n",
       "3            17                 16                   1                      1   \n",
       "4            16                 16                   0                      1   \n",
       "..          ...                ...                 ...                    ...   \n",
       "91           99                 19                  80                      1   \n",
       "92         2000                  0                2000                      0   \n",
       "93         2000                737                1263                      0   \n",
       "94           41                 19                  22                      1   \n",
       "95         3493                263                3230                      1   \n",
       "\n",
       "    targets_remaining  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "..                ...  \n",
       "91                  0  \n",
       "92                  1  \n",
       "93                  1  \n",
       "94                  0  \n",
       "95                  0  \n",
       "\n",
       "[96 rows x 18 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent</th>\n",
       "      <th>grid</th>\n",
       "      <th>param_changed</th>\n",
       "      <th>param_value</th>\n",
       "      <th>alpha</th>\n",
       "      <th>episodes</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>epsilon_decay</th>\n",
       "      <th>gamma</th>\n",
       "      <th>iter</th>\n",
       "      <th>sigma</th>\n",
       "      <th>theta</th>\n",
       "      <th>cumulative_reward</th>\n",
       "      <th>total_steps</th>\n",
       "      <th>total_agent_moves</th>\n",
       "      <th>total_failed_moves</th>\n",
       "      <th>total_targets_reached</th>\n",
       "      <th>targets_remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>gamma</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>gamma</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>epsilon</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>epsilon</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-528.0</td>\n",
       "      <td>99</td>\n",
       "      <td>19</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>sigma</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-14000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>sigma</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9578.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>737</td>\n",
       "      <td>1263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>episodes</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-122.0</td>\n",
       "      <td>41</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>iter</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22822.0</td>\n",
       "      <td>3493</td>\n",
       "      <td>263</td>\n",
       "      <td>3230</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 18 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
