{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37cd82fe",
   "metadata": {},
   "source": [
    "# Setup Experimental Table from experiment_values.json\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "edf9ae46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:00:21.662456Z",
     "start_time": "2025-05-16T11:00:21.660015Z"
    }
   },
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "aef3eca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:00:21.671296Z",
     "start_time": "2025-05-16T11:00:21.663944Z"
    }
   },
   "source": [
    "CONFIG_PATH = \"experiment_values.json\"\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "defaults = config[\"defaults\"]\n",
    "grids = config[\"grids\"]\n",
    "experiments = config[\"experiments\"]\n",
    "\n",
    "all_param_names = set()\n",
    "for agent_params in defaults.values():\n",
    "    all_param_names.update(agent_params.keys())\n",
    "\n",
    "rows = []\n",
    "\n",
    "for agent, sweep in experiments.items():\n",
    "    default_params = defaults[agent]\n",
    "\n",
    "    for grid in grids:\n",
    "        for param_name, values in sweep.items():\n",
    "            for val in values:\n",
    "                params = deepcopy(default_params)\n",
    "                params[param_name] = val\n",
    "\n",
    "                row = {\n",
    "                    \"agent\": agent,\n",
    "                    \"grid\": grid,\n",
    "                    \"param_changed\": param_name,\n",
    "                    \"param_value\": val\n",
    "                }\n",
    "\n",
    "                for pname in sorted(all_param_names):\n",
    "                    row[pname] = params.get(pname, float('nan'))\n",
    "\n",
    "                rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df = df.loc[df[\"agent\"] != \"RandomAgent\"]\n",
    "df.to_csv(\"experiment_results/experiment_table.csv\", index = False)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "26988342",
   "metadata": {},
   "source": [
    "# Run Experiment\n",
    "\n",
    "## This is a copy of train.py with minor updates\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aaaab05e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:00:21.674925Z",
     "start_time": "2025-05-16T11:00:21.672395Z"
    }
   },
   "source": [
    "import json, io, sys, re, importlib, inspect\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "from world.reward_functions import custom_reward_function\n",
    "from world import Environment\n",
    "from agents import BaseAgent\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "3203b3ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:00:21.691054Z",
     "start_time": "2025-05-16T11:00:21.676506Z"
    }
   },
   "source": [
    "import os, json, datetime, random\n",
    "\n",
    "def load_agent(agent_name: str, env: Environment, config: dict):\n",
    "    info = config[agent_name]\n",
    "    mod  = importlib.import_module(info[\"module\"])\n",
    "    cls  = getattr(mod, info[\"class\"])\n",
    "    init_args = info.get(\"init_args\", {})\n",
    "    sig = inspect.signature(cls.__init__)\n",
    "    if 'env' in sig.parameters:\n",
    "        return cls(env=env, **init_args), info[\"train_mode\"], info[\"init_args\"]\n",
    "    else:\n",
    "        return cls(**init_args), info[\"train_mode\"], info[\"init_args\"]\n",
    "\n",
    "def update_agent(agent: BaseAgent, args: Namespace, state, next_state, reward, action):\n",
    "    params = inspect.signature(agent.update).parameters\n",
    "    names  = set(params)\n",
    "    if {\"state\",\"next_state\"}.issubset(names):\n",
    "        agent.update(state=state, next_state=next_state, reward=reward, action=action)\n",
    "    elif {\"next_state\",\"reward\",\"action\"}.issubset(names):\n",
    "        agent.update(next_state=next_state, reward=reward, action=action)\n",
    "    elif {\"state\",\"reward\",\"action\"}.issubset(names):\n",
    "        agent.update(state=state, reward=reward, action=action)\n",
    "    else:\n",
    "        agent.update()\n",
    "\n",
    "def train_and_eval(args: Namespace, config: dict):\n",
    "    start = tuple(args.agent_start_pos)\n",
    "\n",
    "    for grid_fp in args.GRID:\n",
    "        env = Environment(\n",
    "            Path(grid_fp),\n",
    "            args.no_gui,\n",
    "            sigma=args.sigma,\n",
    "            agent_start_pos=start,\n",
    "            reward_fn=custom_reward_function,\n",
    "            target_fps=args.fps,\n",
    "            random_seed=args.random_seed\n",
    "        )\n",
    "        env.reset()\n",
    "        agent, mode, init_args = load_agent(args.agent, env, config)\n",
    "\n",
    "        if mode == \"q_learning\":\n",
    "            #Max difference for convergence check\n",
    "            metrics = {\"iterations\": 0, \"steps_taken\": 0, \"deltas\": [], \"rewards\": []}\n",
    "            delta = 1e-6\n",
    "\n",
    "            for ep in trange(args.episodes, desc=f\"Training {args.agent}\"):\n",
    "                # Save a copy of the current Q-table for convergence check\n",
    "                prev_q_table = {\n",
    "                    s: np.copy(q_values) for s, q_values in agent.q_table.items()\n",
    "                }\n",
    "                state = env.reset()\n",
    "                ep_reward = 0.0\n",
    "                for _ in range(args.iter):\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, terminated, info = env.step(action)\n",
    "                    ep_reward += reward\n",
    "                    if terminated:\n",
    "                        break\n",
    "                    agent.update(state, next_state, reward, info[\"actual_action\"])\n",
    "                    state = next_state\n",
    "\n",
    "                if ep >= args.episodes/4:\n",
    "                    agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "                    agent.alpha = max(agent.alpha_min, agent.alpha * agent.alpha_decay)\n",
    "\n",
    "                common_states = set(agent.q_table.keys()) & set(prev_q_table.keys())\n",
    "                if not common_states:\n",
    "                    max_diff = 1\n",
    "                else:\n",
    "                    max_diff = max(\n",
    "                        np.max(np.abs(agent.q_table[s] - prev_q_table[s]))\n",
    "                        for s in common_states\n",
    "                    )\n",
    "                metrics[\"deltas\"].append(max_diff)\n",
    "                metrics[\"rewards\"].append(ep_reward)\n",
    "                metrics[\"steps_taken\"] = env.world_stats[\"total_steps\"]\n",
    "\n",
    "                if max_diff < delta:\n",
    "                    metrics[\"iterations\"] = ep\n",
    "                    break\n",
    "\n",
    "            if metrics[\"iterations\"] == 0:\n",
    "                metrics[\"iterations\"] = args.episodes\n",
    "\n",
    "            agent.metrics = metrics\n",
    "\n",
    "            agent.eval_mode()\n",
    "\n",
    "        elif mode == \"value_iteration\":\n",
    "            state = env.reset()\n",
    "            for _ in trange(args.iter, desc=f\"[Train] {args.agent}\"):\n",
    "                a  = agent.take_action(state)\n",
    "                ns, r, done, info = env.step(a)\n",
    "                update_agent(agent, args, state, ns, r, info[\"actual_action\"])\n",
    "                state = ns\n",
    "                if done: break\n",
    "            agent.metrics[\"steps_taken\"] = env.world_stats[\"total_steps\"]\n",
    "\n",
    "        elif mode == \"monte_carlo\":\n",
    "            delta = 1e-6\n",
    "\n",
    "            metrics = {\"iterations\": 0, \"steps_taken\": 0, \"deltas\": [], \"rewards\": []}\n",
    "\n",
    "            for episode in trange(args.episodes, desc=f\"Training {args.agent}\"):\n",
    "                prev_q = {s: np.copy(agent.q_table[s]) for s in agent.q_table}\n",
    "\n",
    "                state = env.reset()\n",
    "                terminated = False\n",
    "                ep_reward = 0.0\n",
    "                for _ in range(args.iter):\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, terminated, info = env.step(action)\n",
    "                    ep_reward += reward\n",
    "                    if terminated:\n",
    "                        break\n",
    "                    agent.update(state, action, reward, next_state, False)\n",
    "                    state = next_state\n",
    "\n",
    "                agent.update(state, action, reward, next_state, True)\n",
    "\n",
    "                if episode >= args.episodes/4:\n",
    "                    agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "                    agent.alpha = max(agent.alpha_min, agent.alpha * agent.alpha_decay)\n",
    "\n",
    "                # Convergence check\n",
    "                common_states = set(agent.q_table.keys()) & set(prev_q.keys())\n",
    "                if not common_states:\n",
    "                    max_diff = 1\n",
    "                else:\n",
    "                    max_diff = max(\n",
    "                        np.max(np.abs(agent.q_table[s] - prev_q[s]))\n",
    "                        for s in common_states\n",
    "                    )\n",
    "\n",
    "                metrics[\"deltas\"].append(max_diff)\n",
    "                metrics[\"rewards\"].append(ep_reward)\n",
    "                metrics[\"steps_taken\"] = env.world_stats[\"total_steps\"]\n",
    "\n",
    "                if max_diff < delta:\n",
    "                    metrics[\"iterations\"] = episode\n",
    "                    break\n",
    "\n",
    "            if metrics[\"iterations\"] == 0:\n",
    "                metrics[\"iterations\"] = args.episodes\n",
    "\n",
    "            agent.metrics = metrics\n",
    "            agent.epsilon = 0.0  # Switch to greedy\n",
    "\n",
    "        else:  # iterative / random\n",
    "            state = env.reset()\n",
    "            for _ in trange(args.iter, desc=f\"[Train] {args.agent}\"):\n",
    "                a = agent.take_action(state)\n",
    "                ns, r, done, info = env.step(a)\n",
    "                update_agent(agent, args, state, ns, r, info[\"actual_action\"])\n",
    "                state = ns\n",
    "                if done: break\n",
    "\n",
    "        if hasattr(agent, \"metrics\"):\n",
    "            its = agent.metrics.get(\"iterations\", None)\n",
    "            print(f\"[Metrics] {args.agent} converged in {its} iterations\")\n",
    "            metrics_dir = \"metrics\"\n",
    "            os.makedirs(metrics_dir, exist_ok=True)\n",
    "            grid_name = Path(grid).stem  # Extract just the filename without extension\n",
    "            param_str = \"_\".join(f\"{k}-{v}\" for k, v in init_args.items())\n",
    "            fname = f\"{args.agent}_grid-{grid_name}_{param_str}_{random.randrange(1,9999)}.json\"\n",
    "\n",
    "            path = os.path.join(metrics_dir, fname)\n",
    "        try:\n",
    "            with open(path, \"w\") as mf:\n",
    "                json.dump(agent.metrics, mf, indent=2)\n",
    "                print(f\"[Metrics] Saved convergence data to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Metrics] ERROR saving metrics: {e}\")\n",
    "\n",
    "\n",
    "    # capture evaluation output\n",
    "    buf = io.StringIO()\n",
    "    old_out, old_err = sys.stdout, sys.stderr\n",
    "    sys.stdout, sys.stderr = buf, buf\n",
    "    try:\n",
    "        Environment.evaluate_agent(\n",
    "            Path(args.GRID[0]),\n",
    "            agent,\n",
    "            args.iter,\n",
    "            args.sigma,\n",
    "            agent_start_pos=start,\n",
    "            reward_fn=custom_reward_function,\n",
    "            random_seed=args.random_seed,\n",
    "            show_images=False\n",
    "        )\n",
    "    finally:\n",
    "        sys.stdout, sys.stderr = old_out, old_err\n",
    "\n",
    "    text = buf.getvalue()\n",
    "    metrics = {}\n",
    "    for line in text.splitlines():\n",
    "        m = re.match(r\"\\s*([a-z_]+)\\s*:\\s*([-+]?[0-9]*\\.?[0-9]+)\", line)\n",
    "        if m:\n",
    "            k, v = m.group(1), m.group(2)\n",
    "            metrics[k] = int(v) if v.isdigit() else float(v)\n",
    "    return metrics\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "48e64b4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:05:07.872848Z",
     "start_time": "2025-05-16T11:00:21.692288Z"
    }
   },
   "source": [
    "\n",
    "df = pd.read_csv(\"experiment_results/experimental_table.csv\")\n",
    "base_cfg = json.load(open(\"agent_config.json\"))\n",
    "exp_defs = json.load(open(\"experiment_values.json\"))\n",
    "\n",
    "rows = []\n",
    "for idx, row in df.iterrows():\n",
    "    agent = row[\"agent\"]\n",
    "    grid  = row[\"grid\"]\n",
    "    print(f\"{idx+1}: {agent} on {grid} | {row['param_changed']}={row['param_value']}\")\n",
    "\n",
    "    init_args, cli_args = {}, {}\n",
    "    for c,v in row.items():\n",
    "        if pd.isna(v) or c in {\"agent\",\"grid\",\"param_changed\",\"param_value\"}:\n",
    "            continue\n",
    "        if c in {\"episodes\",\"iter\"}:\n",
    "            cli_args[c] = int(v)\n",
    "        elif c == \"sigma\":\n",
    "            cli_args[c] = float(v)\n",
    "        else:\n",
    "            init_args[c] = float(v)\n",
    "\n",
    "    cfg = deepcopy(base_cfg)\n",
    "    defaults_init = cfg[agent].get(\"init_args\", {})\n",
    "    cfg[agent][\"init_args\"] = {**defaults_init, **init_args}\n",
    "\n",
    "    default_sigma = exp_defs[\"defaults\"][agent].get(\"sigma\", 0.0)\n",
    "    sigma = cli_args.get(\"sigma\", default_sigma)\n",
    "\n",
    "    ns = Namespace(\n",
    "        GRID=[f\"grid_configs/{grid}.npy\"],\n",
    "        agent=agent,\n",
    "        no_gui=True,\n",
    "        sigma=sigma,\n",
    "        fps=5,\n",
    "        episodes=cli_args.get(\"episodes\",\n",
    "                              exp_defs[\"defaults\"][agent].get(\"episodes\",2000)),\n",
    "        iter=cli_args.get(\"iter\",\n",
    "                          exp_defs[\"defaults\"][agent].get(\"iter\",2000)),\n",
    "        random_seed=42,\n",
    "        agent_start_pos=[1,1]\n",
    "    )\n",
    "\n",
    "    metrics = train_and_eval(ns, cfg)\n",
    "\n",
    "    result = row.to_dict()\n",
    "    result.update(metrics)\n",
    "    result[\"sigma\"] = sigma\n",
    "    rows.append(result)\n",
    "\n",
    "out_df = pd.DataFrame(rows)\n",
    "out_df.to_csv(\"result_multi_experiment.csv\", index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: QLearningAgent on A1_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  19%|█▉        | 384/2000 [00:00<00:02, 739.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 384 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.6_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_3298.json\n",
      "2: QLearningAgent on A1_grid | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▌       | 507/2000 [00:00<00:01, 840.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 507 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.99_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7288.json\n",
      "3: QLearningAgent on A1_grid | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  15%|█▌        | 308/2000 [00:00<00:02, 828.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 308 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.3_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_3081.json\n",
      "4: QLearningAgent on A1_grid | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   9%|▉         | 189/2000 [00:00<00:02, 820.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 189 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.5_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7803.json\n",
      "5: QLearningAgent on A1_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  18%|█▊        | 370/2000 [00:00<00:02, 574.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 370 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.5_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_2198.json\n",
      "6: QLearningAgent on A1_grid | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▍       | 494/2000 [00:00<00:01, 810.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 494 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.8_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7135.json\n",
      "7: QLearningAgent on A1_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  26%|██▌       | 513/2000 [00:00<00:01, 810.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 513 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6465.json\n",
      "8: QLearningAgent on A1_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  18%|█▊        | 368/2000 [00:00<00:02, 587.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 368 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_5264.json\n",
      "9: QLearningAgent on A1_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  10%|▉         | 493/5000 [00:00<00:05, 811.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 493 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_2522.json\n",
      "10: QLearningAgent on A1_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  24%|██▎       | 470/2000 [00:00<00:01, 823.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 470 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-A1_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7258.json\n",
      "11: QLearningAgent on Maze | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▌       | 504/2000 [00:00<00:02, 523.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 504 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.6_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_9535.json\n",
      "12: QLearningAgent on Maze | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  26%|██▌       | 513/2000 [00:00<00:02, 553.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 513 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.99_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_887.json\n",
      "13: QLearningAgent on Maze | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▌       | 504/2000 [00:00<00:02, 585.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 504 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.3_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_8794.json\n",
      "14: QLearningAgent on Maze | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  11%|█         | 217/2000 [00:00<00:03, 528.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 217 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.5_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_2467.json\n",
      "15: QLearningAgent on Maze | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  22%|██▏       | 442/2000 [00:01<00:04, 361.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 442 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.5_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7495.json\n",
      "16: QLearningAgent on Maze | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▌       | 507/2000 [00:00<00:02, 530.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 507 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.8_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7245.json\n",
      "17: QLearningAgent on Maze | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▍       | 497/2000 [00:00<00:02, 576.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 497 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6371.json\n",
      "18: QLearningAgent on Maze | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  21%|██▏       | 425/2000 [00:01<00:04, 383.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 425 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_18.json\n",
      "19: QLearningAgent on Maze | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  15%|█▌        | 755/5000 [00:01<00:07, 582.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 755 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_5000.json\n",
      "20: QLearningAgent on Maze | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  25%|██▌       | 508/2000 [00:00<00:02, 554.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 508 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-Maze_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6663.json\n",
      "21: QLearningAgent on test_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   3%|▎         | 69/2000 [00:00<00:01, 1593.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 69 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.6_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6654.json\n",
      "22: QLearningAgent on test_grid | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▍         | 79/2000 [00:00<00:01, 1586.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 79 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.99_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_518.json\n",
      "23: QLearningAgent on test_grid | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   2%|▎         | 50/2000 [00:00<00:01, 1542.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 50 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.3_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_5767.json\n",
      "24: QLearningAgent on test_grid | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   1%|          | 24/2000 [00:00<00:01, 1420.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 24 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.5_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_1170.json\n",
      "25: QLearningAgent on test_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 74/2000 [00:00<00:01, 1461.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 74 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.5_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_6810.json\n",
      "26: QLearningAgent on test_grid | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 71/2000 [00:00<00:01, 1700.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 71 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.8_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_7369.json\n",
      "27: QLearningAgent on test_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▍         | 78/2000 [00:00<00:01, 1721.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 78 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_5941.json\n",
      "28: QLearningAgent on test_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   3%|▎         | 64/2000 [00:00<00:01, 1362.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 64 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_8281.json\n",
      "29: QLearningAgent on test_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   1%|▏         | 73/5000 [00:00<00:03, 1636.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 73 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_9534.json\n",
      "30: QLearningAgent on test_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▍         | 75/2000 [00:00<00:01, 1676.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 75 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-test_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_2656.json\n",
      "31: QLearningAgent on large_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   9%|▉         | 189/2000 [00:00<00:03, 560.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 189 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.6_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_5225.json\n",
      "32: QLearningAgent on large_grid | gamma=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  12%|█▏        | 242/2000 [00:00<00:02, 633.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 242 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.99_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_4529.json\n",
      "33: QLearningAgent on large_grid | alpha=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   7%|▋         | 134/2000 [00:00<00:03, 602.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 134 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.3_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_2403.json\n",
      "34: QLearningAgent on large_grid | alpha=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   7%|▋         | 132/2000 [00:00<00:02, 682.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 132 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.5_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_5617.json\n",
      "35: QLearningAgent on large_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  14%|█▍        | 285/2000 [00:00<00:03, 508.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 285 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.5_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_4858.json\n",
      "36: QLearningAgent on large_grid | epsilon_decay=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  11%|█         | 218/2000 [00:00<00:02, 618.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 218 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.8_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_1320.json\n",
      "37: QLearningAgent on large_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  10%|▉         | 199/2000 [00:00<00:02, 623.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 199 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_437.json\n",
      "38: QLearningAgent on large_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  12%|█▏        | 244/2000 [00:00<00:03, 524.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 244 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_5689.json\n",
      "39: QLearningAgent on large_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▍         | 200/5000 [00:00<00:07, 601.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 200 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_8033.json\n",
      "40: QLearningAgent on large_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  10%|█         | 202/2000 [00:00<00:03, 595.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] QLearningAgent converged in 202 iterations\n",
      "[Metrics] Saved convergence data to metrics\\QLearningAgent_grid-large_grid_gamma-0.9_alpha-0.2_alpha_decay-0.99_alpha_min-0.01_epsilon-0.2_epsilon_decay-0.9_epsilon_min-0.01_num_actions-[0, 1, 2, 3]_5711.json\n",
      "41: ValueIterationAgent on A1_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 16/2000 [00:00<00:00, 29228.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 25 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-06_gamma-0.6_107.json\n",
      "42: ValueIterationAgent on A1_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 15/2000 [00:00<00:00, 30555.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 185 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-06_gamma-0.95_4558.json\n",
      "43: ValueIterationAgent on A1_grid | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 15/2000 [00:00<00:00, 32887.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 82 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-05_gamma-0.9_4558.json\n",
      "44: ValueIterationAgent on A1_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 11/2000 [00:00<00:00, 28746.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 91 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-06_gamma-0.9_435.json\n",
      "45: ValueIterationAgent on A1_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 23/2000 [00:00<00:00, 37845.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 99 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-06_gamma-0.9_8786.json\n",
      "46: ValueIterationAgent on A1_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 15/5000 [00:00<00:00, 27618.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 93 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-A1_grid_theta-1e-06_gamma-0.9_4558.json\n",
      "47: ValueIterationAgent on Maze | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent: 100%|██████████| 2000/2000 [00:00<00:00, 69632.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 24 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-06_gamma-0.6_6228.json\n",
      "48: ValueIterationAgent on Maze | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   2%|▏         | 36/2000 [00:00<00:00, 44620.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 192 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-06_gamma-0.95_9460.json\n",
      "49: ValueIterationAgent on Maze | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   2%|▏         | 36/2000 [00:00<00:00, 43079.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 89 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-05_gamma-0.9_9460.json\n",
      "50: ValueIterationAgent on Maze | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   2%|▏         | 33/2000 [00:00<00:00, 45070.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 97 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-06_gamma-0.9_6202.json\n",
      "51: ValueIterationAgent on Maze | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:  52%|█████▏    | 1047/2000 [00:00<00:00, 62942.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 105 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-06_gamma-0.9_1989.json\n",
      "52: ValueIterationAgent on Maze | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 36/5000 [00:00<00:00, 36131.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 100 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-Maze_theta-1e-06_gamma-0.9_9460.json\n",
      "53: ValueIterationAgent on test_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 14185.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 21 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-06_gamma-0.6_1680.json\n",
      "54: ValueIterationAgent on test_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 8924.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 182 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-06_gamma-0.95_1680.json\n",
      "55: ValueIterationAgent on test_grid | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 10196.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 78 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-05_gamma-0.9_1680.json\n",
      "56: ValueIterationAgent on test_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 2/2000 [00:00<00:00, 8924.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 87 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-06_gamma-0.9_3658.json\n",
      "57: ValueIterationAgent on test_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 5/2000 [00:00<00:00, 15352.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 99 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-06_gamma-0.9_1425.json\n",
      "58: ValueIterationAgent on test_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 3/5000 [00:00<00:00, 7626.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 90 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-test_grid_theta-1e-06_gamma-0.9_1680.json\n",
      "59: ValueIterationAgent on large_grid | gamma=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 41201.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 28 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-06_gamma-0.6_4553.json\n",
      "60: ValueIterationAgent on large_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 36503.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 187 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-06_gamma-0.95_4553.json\n",
      "61: ValueIterationAgent on large_grid | theta=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 38426.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 85 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-05_gamma-0.9_4553.json\n",
      "62: ValueIterationAgent on large_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|          | 16/2000 [00:00<00:00, 35153.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 94 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-06_gamma-0.9_9655.json\n",
      "63: ValueIterationAgent on large_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   1%|▏         | 29/2000 [00:00<00:00, 19408.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 105 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-06_gamma-0.9_9460.json\n",
      "64: ValueIterationAgent on large_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] ValueIterationAgent:   0%|          | 20/5000 [00:00<00:00, 26921.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] ValueIterationAgent converged in 96 iterations\n",
      "[Metrics] Saved convergence data to metrics\\ValueIterationAgent_grid-large_grid_theta-1e-06_gamma-0.9_4553.json\n",
      "65: MCAgentOn on A1_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  94%|█████████▍| 1890/2000 [00:12<00:00, 145.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1890 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.95_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_2381.json\n",
      "66: MCAgentOn on A1_grid | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  95%|█████████▍| 1899/2000 [00:02<00:00, 909.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1899 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.999_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_9148.json\n",
      "67: MCAgentOn on A1_grid | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  91%|█████████ | 1811/2000 [00:01<00:00, 1086.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1811 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.2_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_1758.json\n",
      "68: MCAgentOn on A1_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  86%|████████▋ | 1730/2000 [00:01<00:00, 1138.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1730 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.5_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_41.json\n",
      "69: MCAgentOn on A1_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  78%|███████▊  | 1560/2000 [00:01<00:00, 1160.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1560 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_6953.json\n",
      "70: MCAgentOn on A1_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:02<00:00, 730.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_3890.json\n",
      "71: MCAgentOn on A1_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  52%|█████▏    | 2610/5000 [00:02<00:02, 1031.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2610 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_2310.json\n",
      "72: MCAgentOn on A1_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:02<00:00, 818.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-A1_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_5466.json\n",
      "73: MCAgentOn on Maze | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  95%|█████████▍| 1897/2000 [00:13<00:00, 144.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1897 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.95_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_5349.json\n",
      "74: MCAgentOn on Maze | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:14<00:00, 141.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.999_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_680.json\n",
      "75: MCAgentOn on Maze | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:14<00:00, 142.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.2_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_508.json\n",
      "76: MCAgentOn on Maze | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:13<00:00, 144.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.5_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_1616.json\n",
      "77: MCAgentOn on Maze | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:13<00:00, 147.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_5024.json\n",
      "78: MCAgentOn on Maze | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:15<00:00, 132.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_6356.json\n",
      "79: MCAgentOn on Maze | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 5000/5000 [00:35<00:00, 140.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 5000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_8705.json\n",
      "80: MCAgentOn on Maze | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:35<00:00, 57.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-Maze_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_4846.json\n",
      "81: MCAgentOn on test_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  85%|████████▍ | 1693/2000 [00:00<00:00, 2556.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1693 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.95_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_860.json\n",
      "82: MCAgentOn on test_grid | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  80%|████████  | 1607/2000 [00:00<00:00, 2569.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1607 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.999_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_130.json\n",
      "83: MCAgentOn on test_grid | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  81%|████████  | 1624/2000 [00:00<00:00, 2927.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1624 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.2_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_2986.json\n",
      "84: MCAgentOn on test_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  84%|████████▍ | 1676/2000 [00:00<00:00, 2473.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1676 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.5_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_8548.json\n",
      "85: MCAgentOn on test_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  83%|████████▎ | 1666/2000 [00:00<00:00, 3485.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1666 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_5531.json\n",
      "86: MCAgentOn on test_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  89%|████████▊ | 1772/2000 [00:00<00:00, 1865.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1772 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_6772.json\n",
      "87: MCAgentOn on test_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  48%|████▊     | 2380/5000 [00:00<00:01, 2543.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2380 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_451.json\n",
      "88: MCAgentOn on test_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  81%|████████▏ | 1629/2000 [00:00<00:00, 2670.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 1629 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-test_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_8797.json\n",
      "89: MCAgentOn on large_grid | gamma=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:13<00:00, 144.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.95_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_8682.json\n",
      "90: MCAgentOn on large_grid | gamma=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:03<00:00, 527.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.999_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_9512.json\n",
      "91: MCAgentOn on large_grid | epsilon=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:09<00:00, 205.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.2_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_9148.json\n",
      "92: MCAgentOn on large_grid | epsilon=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:03<00:00, 631.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.5_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_4548.json\n",
      "93: MCAgentOn on large_grid | sigma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:06<00:00, 288.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_4298.json\n",
      "94: MCAgentOn on large_grid | sigma=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:08<00:00, 248.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_5027.json\n",
      "95: MCAgentOn on large_grid | episodes=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn:  55%|█████▌    | 2752/5000 [00:06<00:05, 428.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2752 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_1681.json\n",
      "96: MCAgentOn on large_grid | iter=5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MCAgentOn: 100%|██████████| 2000/2000 [00:22<00:00, 88.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] MCAgentOn converged in 2000 iterations\n",
      "[Metrics] Saved convergence data to metrics\\MCAgentOn_grid-large_grid_gamma-0.99_alpha-0.1_alpha_decay-0.99_alpha_min-1e-08_epsilon-0.1_epsilon_decay-0.99_epsilon_min-0.01_action_space-[0, 1, 2, 3]_4343.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "bbb90598",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:05:07.886632Z",
     "start_time": "2025-05-16T11:05:07.874300Z"
    }
   },
   "source": [
    "out_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             agent        grid param_changed  param_value  alpha  episodes  \\\n",
       "0   QLearningAgent     A1_grid         gamma         0.60    0.2    2000.0   \n",
       "1   QLearningAgent     A1_grid         gamma         0.99    0.2    2000.0   \n",
       "2   QLearningAgent     A1_grid         alpha         0.30    0.3    2000.0   \n",
       "3   QLearningAgent     A1_grid         alpha         0.50    0.5    2000.0   \n",
       "4   QLearningAgent     A1_grid       epsilon         0.50    0.2    2000.0   \n",
       "..             ...         ...           ...          ...    ...       ...   \n",
       "91       MCAgentOn  large_grid       epsilon         0.50    NaN    2000.0   \n",
       "92       MCAgentOn  large_grid         sigma         0.00    NaN    2000.0   \n",
       "93       MCAgentOn  large_grid         sigma         0.50    NaN    2000.0   \n",
       "94       MCAgentOn  large_grid      episodes      5000.00    NaN    5000.0   \n",
       "95       MCAgentOn  large_grid          iter      5000.00    NaN    2000.0   \n",
       "\n",
       "    epsilon  epsilon_decay  gamma  iter  sigma  theta  cumulative_reward  \\\n",
       "0       0.2            0.9   0.60  2000    0.1    NaN               35.0   \n",
       "1       0.2            0.9   0.99  2000    0.1    NaN               28.0   \n",
       "2       0.2            0.9   0.90  2000    0.1    NaN               28.0   \n",
       "3       0.2            0.9   0.90  2000    0.1    NaN               28.0   \n",
       "4       0.5            0.9   0.90  2000    0.1    NaN               28.0   \n",
       "..      ...            ...    ...   ...    ...    ...                ...   \n",
       "91      0.5            NaN   0.99  2000    0.1    NaN             -528.0   \n",
       "92      0.1            NaN   0.99  2000    0.0    NaN           -14000.0   \n",
       "93      0.1            NaN   0.99  2000    0.5    NaN            -9578.0   \n",
       "94      0.1            NaN   0.99  2000    0.1    NaN             -122.0   \n",
       "95      0.1            NaN   0.99  5000    0.1    NaN           -22822.0   \n",
       "\n",
       "    total_steps  total_agent_moves  total_failed_moves  total_targets_reached  \\\n",
       "0            16                 16                   0                      1   \n",
       "1            17                 16                   1                      1   \n",
       "2            17                 16                   1                      1   \n",
       "3            17                 16                   1                      1   \n",
       "4            17                 16                   1                      1   \n",
       "..          ...                ...                 ...                    ...   \n",
       "91           99                 19                  80                      1   \n",
       "92         2000                  0                2000                      0   \n",
       "93         2000                737                1263                      0   \n",
       "94           41                 19                  22                      1   \n",
       "95         3493                263                3230                      1   \n",
       "\n",
       "    targets_remaining  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "..                ...  \n",
       "91                  0  \n",
       "92                  1  \n",
       "93                  1  \n",
       "94                  0  \n",
       "95                  0  \n",
       "\n",
       "[96 rows x 18 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent</th>\n",
       "      <th>grid</th>\n",
       "      <th>param_changed</th>\n",
       "      <th>param_value</th>\n",
       "      <th>alpha</th>\n",
       "      <th>episodes</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>epsilon_decay</th>\n",
       "      <th>gamma</th>\n",
       "      <th>iter</th>\n",
       "      <th>sigma</th>\n",
       "      <th>theta</th>\n",
       "      <th>cumulative_reward</th>\n",
       "      <th>total_steps</th>\n",
       "      <th>total_agent_moves</th>\n",
       "      <th>total_failed_moves</th>\n",
       "      <th>total_targets_reached</th>\n",
       "      <th>targets_remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>gamma</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>gamma</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QLearningAgent</td>\n",
       "      <td>A1_grid</td>\n",
       "      <td>epsilon</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>epsilon</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-528.0</td>\n",
       "      <td>99</td>\n",
       "      <td>19</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>sigma</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-14000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>sigma</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9578.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>737</td>\n",
       "      <td>1263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>episodes</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-122.0</td>\n",
       "      <td>41</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>MCAgentOn</td>\n",
       "      <td>large_grid</td>\n",
       "      <td>iter</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22822.0</td>\n",
       "      <td>3493</td>\n",
       "      <td>263</td>\n",
       "      <td>3230</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 18 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
