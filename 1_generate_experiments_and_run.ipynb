{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1cc144",
   "metadata": {},
   "source": [
    "# Generate experiment csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391b082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG_PATH = \"experiment_values.json\"\n",
    "\n",
    "# with open(CONFIG_PATH) as f:\n",
    "#     config = json.load(f)\n",
    "\n",
    "# defaults = config[\"defaults\"]\n",
    "# grids = config[\"grids\"]\n",
    "# experiments = config[\"experiments\"]\n",
    "\n",
    "# for agent, sweep in experiments.items():\n",
    "# \tdefault_params = defaults[agent]\n",
    "\n",
    "# \tfor grid in grids:\n",
    "# \t\tfor param_name, values in sweep.items():\n",
    "# \t\t\tfor val in values:\n",
    "# \t\t\t\tparams = deepcopy(default_params)\n",
    "# \t\t\t\tparams[param_name] = val\n",
    "\n",
    "# \t\t\t\trun_config = {\n",
    "# \t\t\t\t\t\"agent\": agent,\n",
    "# \t\t\t\t\t\"grid\": grid,\n",
    "# \t\t\t\t\t\"param_changed\": param_name,\n",
    "# \t\t\t\t\t\"param_value\": val,\n",
    "# \t\t\t\t\t\"params\": params\n",
    "# \t\t\t\t}\n",
    "\n",
    "# \t\t\t\tcli_args = []\n",
    "# \t\t\t\tfor k, v in run_config[\"params\"].items():\n",
    "# \t\t\t\t\tcli_args += [f\"--{k}\", str(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747ae68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--iter', '5000']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cli_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7929b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"experiment_values.json\"\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "defaults = config[\"defaults\"]\n",
    "grids = config[\"grids\"]\n",
    "experiments = config[\"experiments\"]\n",
    "\n",
    "all_param_names = set()\n",
    "for agent_params in defaults.values():\n",
    "    all_param_names.update(agent_params.keys())\n",
    "\n",
    "rows = []\n",
    "\n",
    "for agent, sweep in experiments.items():\n",
    "    default_params = defaults[agent]\n",
    "\n",
    "    for grid in grids:\n",
    "        for param_name, values in sweep.items():\n",
    "            for val in values:\n",
    "                params = deepcopy(default_params)\n",
    "                params[param_name] = val\n",
    "\n",
    "                row = {\n",
    "                    \"agent\": agent,\n",
    "                    \"grid\": grid,\n",
    "                    \"param_changed\": param_name,\n",
    "                    \"param_value\": val\n",
    "                }\n",
    "\n",
    "                for pname in sorted(all_param_names):\n",
    "                    row[pname] = params.get(pname, float('nan'))\n",
    "\n",
    "                rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df.to_csv(\"experiment_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"agent\"] != \"RandomAgent\"]\n",
    "df.to_csv(\"experimental_table.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c0d1c",
   "metadata": {},
   "source": [
    "# Training - Copy + Minor update to train.py code. Train.py wasn't touched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82bc673a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from argparse import Namespace\n",
    "from tqdm import trange\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import inspect\n",
    "from inspect import Parameter\n",
    "\n",
    "from world.reward_functions import custom_reward_function\n",
    "from world import Environment\n",
    "from agents import BaseAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577ba2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agent(agent_name: str, env: Environment, config: dict) -> tuple[BaseAgent, str]:\n",
    "    agent_info = config[agent_name]\n",
    "    module = importlib.import_module(agent_info[\"module\"])\n",
    "    AgentClass = getattr(module, agent_info[\"class\"])\n",
    "    init_args = agent_info.get(\"init_args\", {})\n",
    "\n",
    "    sig = inspect.signature(AgentClass.__init__)\n",
    "    if 'env' in sig.parameters:\n",
    "        agent = AgentClass(env=env, **init_args)\n",
    "    else:\n",
    "        agent = AgentClass(**init_args)\n",
    "\n",
    "    return agent, agent_info[\"train_mode\"]\n",
    "\n",
    "def update_agent(agent: BaseAgent, args: Namespace,\n",
    "                 state: tuple[int, int],\n",
    "                 next_state: tuple[int, int],\n",
    "                 reward: float,\n",
    "                 actual_action: int) -> None:\n",
    "    update_params = inspect.signature(agent.update).parameters\n",
    "    update_param_names = list(update_params)\n",
    "\n",
    "    if {\"state\", \"next_state\"}.issubset(update_param_names):\n",
    "        agent.update(state=state, next_state=next_state, reward=reward, action=actual_action)\n",
    "    elif {\"next_state\", \"reward\", \"action\"}.issubset(update_param_names):\n",
    "        agent.update(next_state=next_state, reward=reward, action=actual_action)\n",
    "    elif {\"state\", \"reward\", \"action\"}.issubset(update_param_names):\n",
    "        agent.update(state=state, reward=reward, action=actual_action)\n",
    "    elif all(p.kind in {Parameter.VAR_POSITIONAL, Parameter.VAR_KEYWORD} for p in update_params.values()):\n",
    "        agent.update()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported update() signature: {update_param_names}\")\n",
    "\n",
    "def train_agent(args: Namespace, config: dict):\n",
    "\tstart_pos = tuple(args.agent_start_pos)\n",
    "\tfor grid_path in args.GRID:\n",
    "\t\tenv = Environment(\n",
    "\t\t\tPath(grid_path), args.no_gui, sigma=args.sigma, agent_start_pos=start_pos,\n",
    "\t\t\treward_fn=custom_reward_function, target_fps=args.fps, random_seed=args.random_seed\n",
    "\t\t)\n",
    "\t\tenv.reset()\n",
    "\t\tagent, mode = load_agent(args.agent, env, config)\n",
    "\n",
    "\t\tif mode == \"episodic\":\n",
    "\t\t\tdelta = 1e-6\n",
    "\t\t\tfor ep in trange(args.episodes, desc=f\"Training {args.agent}\"):\n",
    "\t\t\t\tprev_q = {s: np.copy(q) for s, q in agent.q_table.items()}\n",
    "\t\t\t\tstate = env.reset()\n",
    "\t\t\t\tfor _ in range(args.iter):\n",
    "\t\t\t\t\taction = agent.take_action(state)\n",
    "\t\t\t\t\tnext_state, reward, terminated, info = env.step(action)\n",
    "\t\t\t\t\tif terminated: break\n",
    "\t\t\t\t\tagent.update(state, next_state, reward, info[\"actual_action\"])\n",
    "\t\t\t\t\tstate = next_state\n",
    "\t\t\t\tcommon = set(agent.q_table.keys()) & set(prev_q.keys())\n",
    "\t\t\t\tmax_diff = max(\n",
    "\t\t\t\t\tnp.max(np.abs(agent.q_table[s] - prev_q[s]))\n",
    "\t\t\t\t\tfor s in common\n",
    "\t\t\t\t) if common else 10\n",
    "\t\t\t\tif max_diff < delta:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tagent.eval_mode()\n",
    "\n",
    "\t\telif mode == \"iterative\":\n",
    "\t\t\tstate = env.reset()\n",
    "\t\t\tfor _ in trange(args.iter, desc=f\"Training {args.agent}\"):\n",
    "\t\t\t\taction = agent.take_action(state)\n",
    "\t\t\t\tnext_state, reward, terminated, info = env.step(action)\n",
    "\t\t\t\tupdate_agent(agent, args, state, next_state, reward, info[\"actual_action\"])\n",
    "\t\t\t\tstate = next_state\n",
    "\t\t\t\tif terminated: break\n",
    "\n",
    "\t\t# Environment.evaluate_agent(\n",
    "\t\t#     grid_path, agent, args.iter, args.sigma, agent_start_pos=start_pos,\n",
    "\t\t#     reward_fn=custom_reward_function, random_seed=args.random_seed\n",
    "\t\t# )\n",
    "\t\tEnvironment.evaluate_agent(\n",
    "\t\t\tPath(grid_path), agent, args.iter, args.sigma, agent_start_pos=start_pos,\n",
    "\t\t\treward_fn=custom_reward_function, random_seed=args.random_seed\n",
    "\t\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cfe24",
   "metadata": {},
   "source": [
    "### Can just regen df instead of reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df77a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/aniket/TU_Eindhoven/2_Study/Q4_S2AMC15_Data_Intelligence_Challenge/4_Code/2AMC15_Intelligence_Challenge/experimental_table.csv\")  # or assign df directly\n",
    "with open(\"agent_config.json\") as f:\n",
    "    base_config = json.load(f)\n",
    "\n",
    "TRAIN_ARGS = {\"episodes\", \"iter\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d036c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: QLearningAgent on A1_grid | gamma = 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  24%|██▎       | 470/2000 [00:00<00:00, 1620.14it/s]\n",
      "Evaluating agent:   1%|          | 15/2000 [00:00<00:00, 122164.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 35\n",
      "total_steps: 16\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 2: QLearningAgent on A1_grid | gamma = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  29%|██▉       | 578/2000 [00:00<00:00, 1952.64it/s]\n",
      "Evaluating agent:   1%|          | 15/2000 [00:00<00:00, 159681.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 35\n",
      "total_steps: 16\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 3: QLearningAgent on A1_grid | alpha = 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  19%|█▉        | 376/2000 [00:00<00:00, 1971.08it/s]\n",
      "Evaluating agent:   1%|          | 15/2000 [00:00<00:00, 111156.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 35\n",
      "total_steps: 16\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 4: QLearningAgent on A1_grid | alpha = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  11%|█         | 213/2000 [00:00<00:00, 1799.22it/s]\n",
      "Evaluating agent:   1%|          | 16/2000 [00:00<00:00, 137518.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 28\n",
      "total_steps: 17\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 5: QLearningAgent on A1_grid | epsilon = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  28%|██▊       | 564/2000 [00:00<00:00, 1876.20it/s]\n",
      "Evaluating agent:   1%|          | 15/2000 [00:00<00:00, 102134.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 35\n",
      "total_steps: 16\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 6: QLearningAgent on A1_grid | epsilon_decay = 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  29%|██▊       | 573/2000 [00:00<00:00, 1943.71it/s]\n",
      "Evaluating agent:   1%|          | 15/2000 [00:00<00:00, 148734.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 35\n",
      "total_steps: 16\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 7: QLearningAgent on A1_grid | episodes = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  11%|█▏        | 567/5000 [00:00<00:02, 1938.97it/s]\n",
      "Evaluating agent:   1%|          | 16/2000 [00:00<00:00, 161319.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 28\n",
      "total_steps: 17\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 8: QLearningAgent on A1_grid | iter = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  29%|██▉       | 578/2000 [00:00<00:00, 1952.24it/s]\n",
      "Evaluating agent:   0%|          | 15/5000 [00:00<00:00, 156115.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 35\n",
      "total_steps: 16\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 9: QLearningAgent on Maze | gamma = 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  13%|█▎        | 258/2000 [00:00<00:01, 956.43it/s]\n",
      "Evaluating agent:   2%|▏         | 36/2000 [00:00<00:00, 149796.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 8\n",
      "total_steps: 37\n",
      "total_agent_moves: 36\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 10: QLearningAgent on Maze | gamma = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  23%|██▎       | 467/2000 [00:00<00:01, 1308.58it/s]\n",
      "Evaluating agent:   2%|▏         | 36/2000 [00:00<00:00, 229475.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 8\n",
      "total_steps: 37\n",
      "total_agent_moves: 36\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 11: QLearningAgent on Maze | alpha = 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  15%|█▍        | 298/2000 [00:00<00:01, 1270.92it/s]\n",
      "Evaluating agent:   2%|▏         | 36/2000 [00:00<00:00, 154549.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 8\n",
      "total_steps: 37\n",
      "total_agent_moves: 36\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 12: QLearningAgent on Maze | alpha = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   8%|▊         | 168/2000 [00:00<00:01, 1228.38it/s]\n",
      "Evaluating agent:   2%|▏         | 36/2000 [00:00<00:00, 235194.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 8\n",
      "total_steps: 37\n",
      "total_agent_moves: 36\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 13: QLearningAgent on Maze | epsilon = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  21%|██        | 415/2000 [00:00<00:01, 1141.25it/s]\n",
      "Evaluating agent:   2%|▏         | 36/2000 [00:00<00:00, 182802.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 8\n",
      "total_steps: 37\n",
      "total_agent_moves: 36\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 14: QLearningAgent on Maze | epsilon_decay = 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  21%|██        | 416/2000 [00:00<00:01, 1187.66it/s]\n",
      "Evaluating agent:   2%|▏         | 36/2000 [00:00<00:00, 238538.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 8\n",
      "total_steps: 37\n",
      "total_agent_moves: 36\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 15: QLearningAgent on Maze | episodes = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   8%|▊         | 423/5000 [00:00<00:03, 1266.34it/s]\n",
      "Evaluating agent:   2%|▏         | 36/2000 [00:00<00:00, 211774.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 8\n",
      "total_steps: 37\n",
      "total_agent_moves: 36\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 16: QLearningAgent on Maze | iter = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  21%|██        | 423/2000 [00:00<00:01, 1268.97it/s]\n",
      "Evaluating agent:   1%|          | 36/5000 [00:00<00:00, 209134.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 8\n",
      "total_steps: 37\n",
      "total_agent_moves: 36\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 17: QLearningAgent on test_grid | gamma = 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 71/2000 [00:00<00:00, 3982.02it/s]\n",
      "Evaluating agent:   0%|          | 3/2000 [00:00<00:00, 43539.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 18: QLearningAgent on test_grid | gamma = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 73/2000 [00:00<00:00, 3841.76it/s]\n",
      "Evaluating agent:   0%|          | 3/2000 [00:00<00:00, 48960.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 19: QLearningAgent on test_grid | alpha = 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   2%|▏         | 48/2000 [00:00<00:00, 3942.48it/s]\n",
      "Evaluating agent:   0%|          | 3/2000 [00:00<00:00, 42799.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 20: QLearningAgent on test_grid | alpha = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   1%|▏         | 26/2000 [00:00<00:00, 3512.03it/s]\n",
      "Evaluating agent:   0%|          | 3/2000 [00:00<00:00, 55676.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 21: QLearningAgent on test_grid | epsilon = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▍         | 75/2000 [00:00<00:00, 4006.43it/s]\n",
      "Evaluating agent:   0%|          | 3/2000 [00:00<00:00, 57456.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 22: QLearningAgent on test_grid | epsilon_decay = 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 74/2000 [00:00<00:00, 4005.19it/s]\n",
      "Evaluating agent:   0%|          | 3/2000 [00:00<00:00, 42083.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 23: QLearningAgent on test_grid | episodes = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   1%|▏         | 73/5000 [00:00<00:01, 3723.92it/s]\n",
      "Evaluating agent:   0%|          | 3/2000 [00:00<00:00, 47482.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 24: QLearningAgent on test_grid | iter = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▎         | 73/2000 [00:00<00:00, 3760.18it/s]\n",
      "Evaluating agent:   0%|          | 3/5000 [00:00<00:00, 47662.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 25: QLearningAgent on large_grid | gamma = 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   8%|▊         | 153/2000 [00:00<00:01, 1152.28it/s]\n",
      "Evaluating agent:   1%|          | 20/2000 [00:00<00:00, 199728.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 26: QLearningAgent on large_grid | gamma = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  11%|█         | 218/2000 [00:00<00:01, 1360.30it/s]\n",
      "Evaluating agent:   1%|          | 20/2000 [00:00<00:00, 202135.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 27: QLearningAgent on large_grid | alpha = 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   7%|▋         | 144/2000 [00:00<00:01, 1390.21it/s]\n",
      "Evaluating agent:   1%|          | 20/2000 [00:00<00:00, 156212.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 28: QLearningAgent on large_grid | alpha = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▍         | 82/2000 [00:00<00:01, 1305.73it/s]\n",
      "Evaluating agent:   1%|          | 20/2000 [00:00<00:00, 171897.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 29: QLearningAgent on large_grid | epsilon = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  10%|█         | 204/2000 [00:00<00:01, 1179.94it/s]\n",
      "Evaluating agent:   1%|          | 20/2000 [00:00<00:00, 113666.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 30: QLearningAgent on large_grid | epsilon_decay = 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  10%|█         | 204/2000 [00:00<00:01, 1384.28it/s]\n",
      "Evaluating agent:   1%|          | 20/2000 [00:00<00:00, 192399.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 31: QLearningAgent on large_grid | episodes = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:   4%|▍         | 214/5000 [00:00<00:03, 1395.44it/s]\n",
      "Evaluating agent:   1%|          | 20/2000 [00:00<00:00, 178861.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 32: QLearningAgent on large_grid | iter = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training QLearningAgent:  10%|█         | 209/2000 [00:00<00:01, 1377.28it/s]\n",
      "Evaluating agent:   0%|          | 20/5000 [00:00<00:00, 190650.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 33: ValueIterationAgent on A1_grid | gamma = 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   1%|          | 16/2000 [00:00<00:00, 58102.91it/s]\n",
      "Evaluating agent:   1%|          | 16/2000 [00:00<00:00, 250406.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 28\n",
      "total_steps: 17\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 34: ValueIterationAgent on A1_grid | gamma = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   1%|          | 15/2000 [00:00<00:00, 59297.42it/s]\n",
      "Evaluating agent:   1%|          | 15/2000 [00:00<00:00, 207638.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 35\n",
      "total_steps: 16\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 35: ValueIterationAgent on A1_grid | theta = 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   1%|          | 15/2000 [00:00<00:00, 58362.30it/s]\n",
      "Evaluating agent:   1%|          | 15/2000 [00:00<00:00, 219980.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 35\n",
      "total_steps: 16\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 36: ValueIterationAgent on A1_grid | iter = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   0%|          | 15/5000 [00:00<00:00, 61984.79it/s]\n",
      "Evaluating agent:   0%|          | 15/5000 [00:00<00:00, 202950.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 35\n",
      "total_steps: 16\n",
      "total_agent_moves: 16\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 37: ValueIterationAgent on Maze | gamma = 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent: 100%|██████████| 2000/2000 [00:00<00:00, 112636.56it/s]\n",
      "Evaluating agent: 100%|██████████| 2000/2000 [00:00<00:00, 1010188.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: -2168\n",
      "total_steps: 2000\n",
      "total_agent_moves: 1972\n",
      "total_failed_moves: 28\n",
      "total_targets_reached: 0\n",
      "targets_remaining: 1\n",
      "Row 38: ValueIterationAgent on Maze | gamma = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   2%|▏         | 36/2000 [00:00<00:00, 81486.75it/s]\n",
      "Evaluating agent:   2%|▏         | 36/2000 [00:00<00:00, 375609.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 8\n",
      "total_steps: 37\n",
      "total_agent_moves: 36\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 39: ValueIterationAgent on Maze | theta = 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   2%|▏         | 36/2000 [00:00<00:00, 84307.62it/s]\n",
      "Evaluating agent:   2%|▏         | 36/2000 [00:00<00:00, 404812.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 8\n",
      "total_steps: 37\n",
      "total_agent_moves: 36\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 40: ValueIterationAgent on Maze | iter = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   1%|          | 36/5000 [00:00<00:00, 84307.62it/s]\n",
      "Evaluating agent:   1%|          | 36/5000 [00:00<00:00, 405900.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 8\n",
      "total_steps: 37\n",
      "total_agent_moves: 36\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 41: ValueIterationAgent on test_grid | gamma = 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 19388.15it/s]\n",
      "Evaluating agent:   0%|          | 3/2000 [00:00<00:00, 53317.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 42: ValueIterationAgent on test_grid | gamma = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 21620.12it/s]\n",
      "Evaluating agent:   0%|          | 3/2000 [00:00<00:00, 66576.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 43: ValueIterationAgent on test_grid | theta = 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   0%|          | 3/2000 [00:00<00:00, 23652.09it/s]\n",
      "Evaluating agent:   0%|          | 3/2000 [00:00<00:00, 58798.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 44: ValueIterationAgent on test_grid | iter = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   0%|          | 3/5000 [00:00<00:00, 22231.29it/s]\n",
      "Evaluating agent:   0%|          | 3/5000 [00:00<00:00, 54471.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 41\n",
      "total_steps: 4\n",
      "total_agent_moves: 3\n",
      "total_failed_moves: 1\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 45: ValueIterationAgent on large_grid | gamma = 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 66894.80it/s]\n",
      "Evaluating agent:   1%|          | 20/2000 [00:00<00:00, 282444.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 46: ValueIterationAgent on large_grid | gamma = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 65128.94it/s]\n",
      "Evaluating agent:   1%|          | 20/2000 [00:00<00:00, 286300.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 47: ValueIterationAgent on large_grid | theta = 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   1%|          | 20/2000 [00:00<00:00, 68478.43it/s]\n",
      "Evaluating agent:   1%|          | 20/2000 [00:00<00:00, 290263.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n",
      "Row 48: ValueIterationAgent on large_grid | iter = 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ValueIterationAgent:   0%|          | 20/5000 [00:00<00:00, 67324.30it/s]\n",
      "Evaluating agent:   0%|          | 20/5000 [00:00<00:00, 274137.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results:\n",
      "cumulative_reward: 30\n",
      "total_steps: 21\n",
      "total_agent_moves: 21\n",
      "total_failed_moves: 0\n",
      "total_targets_reached: 1\n",
      "targets_remaining: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in df.iterrows():\n",
    "    agent = row[\"agent\"]\n",
    "    grid = row[\"grid\"]\n",
    "\n",
    "    print(f\"Row {i+1}: {agent} on {grid} | {row['param_changed']} = {row['param_value']}\")\n",
    "\n",
    "    init_args = {}\n",
    "    train_args = {}\n",
    "\n",
    "    for col, val in row.items():\n",
    "        if pd.isna(val): continue\n",
    "        if col in {\"agent\", \"grid\", \"param_changed\", \"param_value\"}:\n",
    "            continue\n",
    "        if col in TRAIN_ARGS:\n",
    "            train_args[col] = int(val)\n",
    "        else:\n",
    "            init_args[col] = float(val) if isinstance(val, float) else val\n",
    "\n",
    "    config = deepcopy(base_config)\n",
    "    config[agent][\"init_args\"] = init_args\n",
    "    args = Namespace(\n",
    "        GRID=[f\"grid_configs/{grid}.npy\"],\n",
    "        agent=agent,\n",
    "        no_gui=True,\n",
    "        sigma=0.1,\n",
    "        fps=5,\n",
    "        episodes=train_args.get(\"episodes\", 2000),\n",
    "        iter=train_args.get(\"iter\", 2000),\n",
    "        random_seed=42,\n",
    "        agent_start_pos=[1, 1],\n",
    "    )\n",
    "\n",
    "    train_agent(args, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4c5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
